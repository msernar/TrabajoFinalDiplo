---
title: "Trabajo Final DiplomaCSC - Opción 2"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

## Carga de librerías a utilizar

```{r library}
library(tidyverse)
library(tidytext)
library(tm)
library(textstem)
library(patchwork)
library(wordcloud) 
library(plotly)
library(topicmodels)
library(tictoc)
library(cowplot)
library(reshape2)
```

## Etapa 1: carga de datos y preprocesamiento

El archivo que se adjunta consiste en un corpus de unas 7.000 noticias scrapeadas entre julio y septiembre de 2019 de los siguientes medios de circulación nacional:

Télam La Nación Clarín Perfil Infobae MinutoUno Página 12

Constituye una muestra aleatoria del corpus construido por Florencia Piñeyrúa para su tesina de grado “Procesamiento del lenguaje natural aplicado al estudio de tópicos de noticias de seguridad en Argentina: julio a septiembre 2019”. Una exposición más concentrada de sus resultados puede encontrarse en el siguiente artículo.

El corpus contiene las siguientes variables:

id : identificador de cada documento
url : link a la noticia original
fecha : fecha de publicación
anio : año de publicación
mes : mes de publicación
dia : dia de publicación
medio : medio en el que fue publicado
orientacion: clasificación -provisoria- de los medios según su línea
editorial predominante (más conservador, más progresista, neutral)
titulo
texto

### Carga dataset

```{r readcsv}
corpus_base <- read_csv("M5_corpus_medios.csv")
```

```{r analisis_base}
# En este apartado vemos cuántas noticias aporta cada medio al corpus y calculamos la proporción sobre el total.

    corpus_base %>%
        group_by(medio) %>%
        summarise(n=n()) %>%
        mutate(
                total = sum(n),
                prop = n/total*100
                ) %>%
        ungroup() %>%
        select(medio, n, prop) %>% 
        arrange(desc(n))
```

### Normalización y tokenización

```{r tokens}
corpus_tidy <- corpus_base %>% 
    mutate(texto= stringi::stri_trans_general(texto, "Latin-ASCII"),
         titulo = stringi::stri_trans_general(titulo, "Latin-ASCII")) %>% 
    mutate(texto = str_replace_all(texto, '[[:digit:]]+', '')) %>% 
    unnest_tokens(word, texto, to_lower = TRUE)

# En este este código tomamos el corpus de texto "corpus_base", lo normalizamos convirtiendo el texto y el título a caracteres ASCII, eliminamos los dígitos del texto y finalmente los dividimos en tokens para su posterior análisis.
```

### Eliminar stopwords

```{r stopwords}
stop_words_1 <- read_csv('https://raw.githubusercontent.com/Alir3z4/stop-words/master/spanish.txt', col_names=FALSE) %>%
        rename(word = X1) %>%
        mutate(word = stringi::stri_trans_general(word, "Latin-ASCII"))

# En este código leemos un archivo CSV de stop words en español ubicado en la URL especificada, renombramos la columna a "word" y luego transformamos las palabras en la columna para asegurarnos de que estén en formato ASCII. El resultado final es un dataframe llamado "stop_words_1" que contiene la lista de stopwords normalizadas.

stop_words_2 <- read_csv("z_stopwords.txt", col_names = FALSE) %>% 
  rename(word = X1) %>% 
   mutate(word=stringi::stri_trans_general(word, "Latin-ASCII"))

# Ídem pero el archivo CSV llamado "z_stopwords.txt" se carga desde la carpeta.

stop_words_full <- stop_words_1 %>% 
  bind_rows(stop_words_2) %>% 
  distinct()

# En este paso combinamos los dos dataframes de stopwords, eliminamos las filas duplicadas y guardamos el resultado en un nuevo dataframe llamado "stop_words_full". Este dataframe contiene la lista completa y única de stopwords normalizadas.

corpus_tidy <- corpus_tidy %>% 
  anti_join(stop_words_full)

# En este código eliminamos las stopwords del corpus de texto "corpus_tidy" utilizando la lista de stopwords contenida en el dataframe "stop_words_full". Este paso en el procesamiento de texto es necesario para eliminar palabras que no aportan significado para el análisis posterior.
```

### Corrección "años"

```{r correccion}
corpus_tidy <- corpus_tidy %>% 
  mutate(word = case_when(
    word == 'ano' ~ 'anio',
    word == 'anos' ~ 'anio',
    TRUE ~ word
  ))
```

## Etapa 2: Consignas

¿Cuáles son las palabras más utilizadas en cada uno de los medios? ¿Pueden verse diferencias? (Tener en cuenta las diferentes métricas trabajadas en el curso: tf, tf-idf, etc.) Generar las visualizaciones que considere más pertinentes para responder la pregunta.

### Exploración palabras más frecuentes

Comenzamos explorando el corpus en formato tidy para identificar rápidamente algunos de los términos más frecuentes por medio.

```{r explore_freqs}
corpus_tidy %>%
        group_by(word, medio) %>%
        summarise(n=n()) %>%
        arrange(desc(n))

# Mediante este código agrupamos las palabras únicas en el corpus, contamos el número de ocurrencias de cada palabra y las ordenamos en función de su frecuencia de aparición, mostrando primero las palabras más frecuentes.

corpus_tidy %>%
  filter(medio == 'infobae')  %>% 
  group_by(word) %>%
  summarise(n=n()) %>%
  arrange(desc(n))
        
# Mirada rápida a las palabras más frecuentes de algunos medios

corpus_tidy %>%
        group_by(medio, word) %>%
        summarise(n=n()) %>%
        arrange(desc(n)) %>%
        pivot_wider(names_from = medio,
                    values_from = n)

# Este código calcula el recuento de ocurrencias de cada palabra en el corpus de texto agrupado según el medio de comunicación, y luego reorganiza estos resultados en un formato más ancho donde cada medio tiene sus recuentos de ocurrencias asociados para cada palabra.

```

```{r word_counts}
word_counts_10 <- corpus_tidy %>%
        group_by(medio, word) %>%
        summarise(n=n()) %>%
        slice_max(n, n = 10) %>% 
        ungroup()

# En este paso calculamos el recuento de ocurrencias de cada palabra en el corpus, agrupadas por la columna "medio", y almacena estos recuentos en un nuevo dataframe llamado "word_counts" para pasarle a la función de graficado.

word_counts_all_10 <- corpus_tidy %>%
        group_by(word) %>%
        summarise(n=n()) %>%
        slice_max(n, n = 10) %>% 
        ungroup() %>% 
        mutate(medio = 'general')

# Aquí calculamos el recuento de ocurrencias de cada palabra en el corpus, sin agrupar por medio. Luego, seleccionamos las 10 palabras más comunes en todo el corpus. A estos resultados les agregamos la etiqueta "general" en la columna "medio" para poder comparar visualmente la distribución general con la distribución por medio

word_counts_top_10 <- word_counts_10 %>% 
  bind_rows(word_counts_all_10)

# Unifico en un dataset
```

A continuación creamos una función para automatizar todas las visualizaciones relacionadas a las métricas del corpus. 

```{r crear_viz}
crear_graf_words <- function(data) {
  medios <- unique(data$medio)
  graficos <- list()
  
  for (medio_actual in medios) {
    datos_medio <- data %>%
      filter(medio == medio_actual) %>%
      mutate(word = fct_reorder(word, n))
    
    grafico <- ggplot(datos_medio, aes(n, word)) +
                geom_col() +
                geom_text(aes(label = n), position = position_stack(vjust = 0.5), family = "Courier", color = "white") +
                labs(title = medio_actual,
                     x = "Frecuencia",
                     y = "Palabra") +
                theme_classic()+
                theme(plot.title = element_text(hjust= 0.5),
                      axis.title = element_blank(),
                      axis.ticks.x = element_blank(),
                      text = element_text(family = "Courier"))
    
    graficos[[medio_actual]] <- grafico
  }
  
  wrap_plots(graficos)
}

# Esta función crea para cada medio un gráfico con el top 10 de palabras según cada métrica analizada

```

El eje x muestra las palabras y el eje y muestra la frecuencia de cada palabra. El gráfico está segmentado en paneles por cada medio, y las escalas del eje y se ajustan para cada panel individual.

```{r viz}
crear_graf_words(word_counts_top_10)

# Este código genera una visualización de las palabras más comunes en el corpus, desglosadas por medio y en general. Las palabras más comunes se muestran en orden descendente de frecuencia en cada categoría.
```

Algunas observaciones preliminares:

-   Clarín e Infobae aportan cada uno el 22% de las noticias al corpus. El peso de las palabras más importantes para estos medios en el ranking general es alto.

-   Año parece una stopword porque es una palabra que presenta una frecuencia alta intra e inter medios.

-   Crónica, LN e Infobae son los únicos que no incluyen nombres propios
entre las palabras más frecuentes.

-   En LN hay muchas referencias a redes sociales y acciones en ellas (guardar, compartir). Las acciones y "fuente" podrían ser stopwords. Incluso las menciones a RRSS podrían ser producto del scrapping y no del contenido de las notas.

-   Macri aparece como el político más nombrado. Lógicamente, es un resultado esperable si se tiene en cuenta que las notas relevadas corresponden al último año de su gobierno. 

-  Infobae y sobre todo Crónica parecen cubrir temas más generales que los demás medios, al incluir palabras como "polícía", "ciudad", "mujer", "hombre", "casa", "personas", "mundo", "vida". Incluso, la frecuencia de la palabra "policía" en Crónica (#2 o #1 si se excluye el término "anio") podría indicar que la mayor parte de la cobertura de este medio se orienta a la temática policial. De todas formas, en el top 10 también aparecen palabras como "gobierno" y "presidente".

-   La palabra "frente" aparece con frecuencia en varios medios, lo que nos inclina a corroborar si refiere a frentes electorales o simplemente a un adverbio de lugar y, en este caso, debería considerarse como stopword.

-   A modo de síntesis, podría advertirse que las palabras con mayor frecuencia de aparición en la totalidad de los medios se corresponden con la temática política. Es probable que esta vinculación sea un desprendimiento del escenario electoral que signó el período bajo análisis.

### Term Frequency - TF:

A modo de práctica generamos el cálculo de term frequency manualmente para visualizar la distribución de términos según su frecuencia respecto al total de términos, aperturado por medio.

```{r words_tidy}
# La "term frequency" (frecuencia de término) es una métrica para evaluar la importancia de una palabra en un documento o corpus de documentos, a partir de la medición de la frecuencia de su aparición en comparación con el número total de palabras.

words_tidy <- corpus_tidy %>% 
  group_by(medio, word) %>% # en función de la consigna, hacemos el conteo de términos por medio
  summarise(n=n()) %>% 
  arrange(desc(n)) 

total_words <- words_tidy %>%
        group_by(medio) %>%
        summarize(total = sum(n)) 

words_tidy <- words_tidy %>% 
  left_join(total_words) %>% 
  ungroup() %>% 
  arrange(desc(n))

# En este código contamos la frecuencia de cada palabra en el corpus del medio y también el total de palabras en el medio. Con estos datos podemos calcular a continuación la importancia de cada término en el medio.
```

## PRUEBITASSS
```{r}
words_tidy_test <- corpus_tidy %>% 
  group_by(id, word, medio) %>% 
  summarise(n=n()) %>% 
  arrange(desc(n)) 

total_words_test <- words_tidy_test %>%
        group_by(id) %>%
        summarize(total = sum(n)) 

words_tidy_test <- words_tidy_test %>% 
  left_join(total_words_test) %>% 
  ungroup() %>% 
  arrange(desc(n))

t <- words_tidy_test %>% mutate(tf = n/total) %>%
          ggplot(aes(tf)) +
                geom_histogram(show.legend = FALSE) +
                coord_flip()+
                #xlim(NA, 0.0002) +
                facet_wrap(~medio) +
                theme_classic()+
                theme(plot.title = element_text(hjust= 0.5),
                      axis.title = element_blank(),
                      axis.ticks.x = element_blank(),
                      text = element_text(family = "Courier"))
                  

```
En esta visualización podemos interactuar para identificar la cantidad de términos por tf:

```{r tf_viz_manual}
tf_viz <- words_tidy %>% mutate(tf = n/total) %>%
          ggplot(aes(tf)) +
                geom_histogram(show.legend = FALSE) +
                coord_flip()+
                xlim(NA, 0.0002) +
                facet_wrap(~medio) +
                theme_classic()+
                theme(plot.title = element_text(hjust= 0.5),
                      axis.title = element_blank(),
                      axis.ticks.x = element_blank(),
                      text = element_text(family = "Courier"))
                  
ggplotly(tf_viz)

```

Vemos que para los medios más masivos se cumple la Ley de Zipf: pocas palabras ocurren muchas veces. Los corpus de Crónica, Minuto Uno y Télam tienen menos términos que los demás y la tf tiene un rango menor (uso de palabras mejor distribuido).

### Term Frequency - Inverse Document Frequency (TF-IDF)

En esta etapa complementamos el análisis con las métricas de idf y tf_idf, para obtener una medida más completa de la importancia de los términos en el corpus de documentos.

```{r tf_idf}
tf_idf <- words_tidy %>% 
  bind_tf_idf(word, medio, n) 

tf_idf %>% select(-total) %>% 
  arrange(desc(tf_idf))

# En este código calculamos las métricas tf, idf y tf_idf para todos los términos del corpus
```

Creamos tres dataset, uno por métrica, con el top ten de términos para pasarle a la función de visualización.

```{r top_ten_metricas}
tf_10 <- tf_idf %>%
        group_by(medio) %>%
        slice_max(tf, n = 10) %>% 
        select(medio, word, tf) %>% 
        rename(n = tf) %>%
        mutate(n = log(n + 1) * 100) %>% 
        mutate(n = round(n, 4))

idf_10 <- tf_idf %>%
          group_by(medio) %>%
          arrange(desc(idf)) %>%
          slice_head(n = 10) %>%
          select(medio, word, idf) %>%
          rename(n = idf) %>%
          mutate(n = log(n + 1) * 100) %>% 
          mutate(n = round(n, 4))

tf_idf_10 <- tf_idf %>%
        group_by(medio) %>%
        arrange(desc(idf)) %>%
        slice_head(n = 10) %>% 
        select(medio, word, tf_idf) %>% 
        rename(n = tf_idf) %>%
        mutate(n = log(n + 1) * 100) %>% 
        mutate(n = round(n, 4))
```

Pasamos cada uno de los top ten de términos por la función de graficado para visualizar los términos más importantes según cada métrica.

```{r tf_viz}
crear_graf_words(tf_10)
```

```{r idf_viz}
crear_graf_words(idf_10)
```

```{r tf_idf_viz}
crear_graf_words(tf_idf_10)
```

Análisis preliminar de TF-IDF:

-   En primer lugar identificamos la necesidad de agregar palabras al
    listado de stopwords. Entendemos que no aportan información sobre el
    contenido u orientación de las noticias y a su vez son únicos para
    algunos medios y eso genera un alto tf_idf.

    Ejemplos: jpe, ap, emj, cronica.com.ar, fvazquez, cronicavirales,
    hd, pemex, gt, afv, jpg, minutouno.com, ambito.com, ivanovich,
    loading, paginai, protected, lxs, email, r.c,cp, fel,l.l,d.s, ea,
    f.f, f.d.s, fh, a.g, pct, telam.la, nacional.el

-   Podemos identificar "firmas" de periodistas (nombres de usuario o
    siglas) que tampoco son informativas sobre el contenido del corpus.

## Etapa 1.2: Ampliación stopwords y reprocesamiento de la base

El análisis realizado nos permitió encontrar numerosas palabras incluidas en el corpus que creemos son producto del scrapping. Previo a la modelización para identificar tópicos creemos pertinente ampliar el listado de stopwords y repetir el preprocesamiento de los datos. 

Para ampliar el listado de stopwords partimos del listado de palabras con sus métricas de tf e idf.

```{r stopwords_v2}
# Mediante el siguiente código eliminamos palabras que incluyen puntos y parecen producto de scrapping web. 
nuevas_stopwords_1 <- tf_idf %>% 
  select(word) %>% 
  filter(grepl('\\.', word)) %>% 
  pull(word)

# En este paso eliminamos palabras de dos caracteres, salvo contadas excepciones que tienen sentido.
nuevas_stopwords_2 <- tf_idf %>% 
  select(word) %>% 
  filter(str_length(word) == 2 & !(word %in% c('pj', 'fe', 'dt', 'tv', 'km', 'cv', 'dr', 'it', 'dj', 'ux'))) %>% 
  pull(word)

# Este código elimina palabras de tres caracteres. Parece haber más siglas y palabras cortas que tienen sentido pero, dada la gran cantidad de stopwords de 3 caracteres, hay motivos para incluir este paso.
nuevas_stopwords_3 <- tf_idf %>% 
  select(word) %>% 
  filter(str_length(word) == 3 & !(word %in% c('san', 'mil', 'ley', 'afp', 'sur', 'fmi', 'usd', 'rio', 'gol', 'paz', 'mar', 'oro', 'red', 'luz', 'voz' , 'rol', 'sol', 'gas', 'pie', 'par', 'pro', 'via', 'onu', 'ypf', 'iva', 'afa', 'pbi', 'bar', 'cfk', 'eje', 'rey', 'atp', 'don', 'fbi', 'gay', 'psg', 'uba', 'ucr', 'ceo', 'ong', 'fed', 'ojo', 'dea', 'uva', 'cgt', 'ufi', 'app', 'gil', 'vih', 'nba', 'bbc', 'evo', 'hip', 'hop', 'fox', 'nbc', 'rap', 'adn', 'ala', 'eva', 'pan', 'zen', 'afv', 'cck', 'eco', 'oca', 'tio', 'cnn', 'cia', "dni", "uia", "fdt", "uif", "hbo", "mls", "oea", "mep", "gnc", "auh", "che", "oil", "gen", "agn",	"fpv", "lam",	"pib", "cne","duo", "vox", "dow", "uca", "dnu", "pez", "pfa", "pdt", "fda", "fci", "oms", "psa", "feo", "cta", "ego",				
"faa",	"ute", "ate",	"lio", "fpt", "suv", "gba",	"izq", "aro", "smn", "tnt",	"uco", "ipc", "saa", "tmz", "ccl", "gel", "vip", "esi",	"res", "kun", "tsj", "afi",	"pts", "cnh", "ajo", "acv", "bmw", "bus", "gps", "ile",	"ios", "unc",	"zoo", "jup", "tos", "unl",	"upl", "zeo", "ave", "mte", "mpn", "apn", "mao", "pba", "sms", "cnv", "mdz", "fol", "iso"))) %>% 
  pull(word)

nuevas_stopwords <- data.frame(word = nuevas_stopwords_1) %>% 
  bind_rows(data.frame(word = nuevas_stopwords_2)) %>% 
  bind_rows(data.frame(word = nuevas_stopwords_3))

stop_words_full_v2 <- stop_words_full %>% 
  bind_rows(tibble(word = c('embed', 'anio', 'ano', 'anos', 'gusta', 'twitter', 'facebook', 'comentar', 'fuente', 'whatsapp', 'guardar', 'compartir', 'mail', 'loading', 'email', 'paginai', 'eltrece', 'infobae', 'http', 'https', 'attribute', 'find_all', 'nonetype', 'object', 'read' ))) %>% 
  bind_rows(nuevas_stopwords)
```

A partir de la nueva lista de stopwords, recreamos el corpus en formato tidy y también el dataframe en formato medio/término/n para continuar con el análisis de métricas.

```{r corpus_tidy_v2}
corpus_tidy_v2 <- corpus_tidy %>% 
  anti_join(stop_words_full_v2)
```

```{r words_tidy_v2} 
words_tidy_v2 <- corpus_tidy_v2 %>% 
  group_by(medio, word) %>%
  summarise(n=n())

total_words_v2 <- words_tidy_v2 %>%
        group_by(medio) %>%
        summarize(total = sum(n)) 

words_tidy_v2 <- words_tidy_v2 %>% 
  left_join(total_words_v2) %>% 
  ungroup() %>% 
  arrange(desc(n))
```

### Revisión de métricas

Creamos el gráfico con la distribución de términos según su tf para identificar variaciones respecto a la v1.

```{r tf_viz_manual_v2}
tf_viz_v2 <- words_tidy_v2 %>% mutate(tf = n/total) %>%
          ggplot(aes(tf)) +
                geom_histogram(show.legend = FALSE) +
                coord_flip()+
                xlim(NA, 0.0002) +
                facet_wrap(~medio) +
                theme_classic()+
                theme(plot.title = element_text(hjust= 0.5),
                      axis.title = element_blank(),
                      axis.ticks.x = element_blank(),
                      text = element_text(family = "Courier"))
                  
ggplotly(tf_viz_v2)
```


```{r tf_idf_v2}
tf_idf_v2 <- words_tidy_v2 %>% 
  bind_tf_idf(word, medio, n) 
```

Creamos los dataset necesarios para visualizar los principales términos por métrica y por medio, buscando diferencias respecto a la v1.

```{r top_ten_metricas_v2}
tf_10_v2 <- tf_idf_v2 %>%
        group_by(medio) %>%
        arrange(desc(tf)) %>%
        slice_head(n = 10) %>%
        select(medio, word, tf) %>% 
        rename(n = tf) %>%
        mutate(n = log(n + 1) * 100) %>% 
        mutate(n = round(n, 4))

idf_10_v2 <- tf_idf_v2 %>%
          group_by(medio) %>%
          arrange(desc(idf)) %>%
          slice_head(n = 10) %>%
          select(medio, word, idf) %>%
          rename(n = idf) %>%
          mutate(n = log(n + 1) * 100) %>% 
          mutate(n = round(n, 4))

tf_idf_10_v2 <- tf_idf_v2 %>%
        group_by(medio) %>%
        arrange(desc(idf)) %>%
        slice_head(n = 10) %>% 
        select(medio, word, tf_idf) %>% 
        rename(n = tf_idf) %>%
        mutate(n = log(n + 1) * 100) %>% 
        mutate(n = round(n, 4))
```

```{r tf_viz_v2}
crear_graf_words(tf_10_v2) 

crear_graf_words(tf_10)
```

```{r idf_viz_v2}
crear_graf_words(idf_10_v2)
```

```{r tf_idf_viz_v2}
crear_graf_words(tf_idf_10_v2)
```

Análisis final de TF-IDF:

-   En todos los medios, excepto en La Nación que no la incluía, logró eliminarse la palabra "anio" que figuraba en el primer lugar en términos de frecuencia. Además, en los casos de Crónica y Minuto 1, se eliminó otra palabra adicional incorporando, de esa manera, dos nuevas al top diez. 

- El caso de La Nación ya se muestra normalizado, sin las stopwords que aparecían producto del scraping.

------------------------------------------------------------------------
## Modelado de tópicos: LDA

¿Cuáles son los tópicos principales en el corpus? ¿Pueden evidenciar diferencias en cada uno de los medios? Explicar qué método se utilizó para responder la pregunta, cuáles son los supuestos del mismo. Generar las visualizaciones más adecuadas para responder a las preguntas.

Para implementar un modelado de temas con LDA necesitamos construir una matriz DTM.

# A PARTIR DE ACÁ
dejo las dos pruebas para que puedas comparar. Si usamos la matriz término-frecuencia por MEDIO, la detección de tópicos da cualquier cosa. Si usamos la matriz término-frecuencia por NOTICIA el resultado tiene más sentido. No estoy segura como se explica, quizás podemos encontrar alguna justificación. 

--> Nos quedamos con la versión 2 (por id)

```{r dtm_1}
disc_dtm_1 <- words_tidy_v2 %>%
                cast_dtm(medio, word, n)

```


```{r dtm_2}
para_disc_dtm <- corpus_tidy_v2 %>% 
  group_by(id, word) %>%
  summarise(n=n())
# creo un conteo de palabras por noticia, no por medio 

disc_dtm_2 <- para_disc_dtm %>%
                cast_dtm(id, word, n)

```

```{r LDA_original}
lda_orig_5 <- LDA(disc_dtm_1, k = 5, control = list(seed = 1234))  # k es el número de tópicos a identificar

ap_topics <- tidy(lda_orig_5, matrix = "beta") 
# la función tidy convierte el modelo a un tópico-término por fila

ap_topics %>%
  mutate(beta = round(100*beta,6))

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales='free_y') +
  scale_y_reordered() +
  theme_minimal()
```

```{r}
lda_v2_5 <- LDA(disc_dtm_2, k = 5, control = list(seed = 555))  # k es el número de tópicos a identificar

ap_topics_5 <- tidy(lda_v2_5, matrix = "beta") 
# la función tidy convierte el modelo a un tópico-término por fila

ap_topics_5 %>%
  mutate(beta = round(100*beta,6))

ap_top_terms_5 <- ap_topics_5 %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms_5 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales='free_y') +
  scale_y_reordered() +
  theme_minimal()
```
Pareciera que los cinco tópicos arrojados como resultado tienen un sentido definido:

1- Política Nacional/ Elecciones
2- Deportes
3- Sociedad
4- Política Internacinal
5- Economía

A continuación, hacemos una nueva prueba para explorar si es posible identificar una mayor cantidad de tópicos (k=10):

```{r}
lda_v2_10 <- LDA(disc_dtm_2, k = 10, control = list(seed = 1010))  # k es el número de tópicos a identificar

ap_topics_10 <- tidy(lda_v2_10, matrix = "beta") 
# la función tidy convierte el modelo a un tópico-término por fila

ap_topics_10 %>%
  mutate(beta = round(100*beta,6))

ap_top_terms_10 <- ap_topics_10 %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms_10 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales='free_y') +
  scale_y_reordered() +
  theme_minimal()
```
Con esta prueba, que duplicó la cantidad de tópicos, comenzamos a registrar conjuntos que no tienen un sentido tan definido, catalogados provisoriamente como tópicos de "Sociedad" (1, 3 y 6). A su vez, en dos casos es posible reconocer una superposición temática (4 y 8).

1- Sociedad
2- Política Nacional/ Elecciones
3- Sociedad (¿Vida Familiar?)
4- Política Internacional
5- Policiales
6- Sociedad
7- Deporte
8- Política Internacional
9- Economía
10- Cultura/ Cine y Series

En esa dirección, avanzamos en una nueva prueba que propone reducir levemente la cantidad de tópicos (k=8):

```{r}
lda_v2_8 <- LDA(disc_dtm_2, k = 8, control = list(seed = 888))  # k es el número de tópicos a identificar

ap_topics_8 <- tidy(lda_v2_8, matrix = "beta") 
# la función tidy convierte el modelo a un tópico-término por fila

ap_topics_8 %>%
  mutate(beta = round(100*beta,6))

ap_top_terms_8 <- ap_topics_8 %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms_8 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales='free_y') +
  scale_y_reordered() +
  theme_minimal()
```
En comparación con la prueba anterior, también detectamos tres tópicos con un sentido difuso (1, 3 y 7), mientras que desaparecieron tanto la repetición del tópico "Política Internacional" como el tópico "Cultura/ Cine y Series".

1- Sociedad
2- Economía
3- Sociedad
4- Policiales
5- Política Internacional
6- Deportes
7- Sociedad (¿Y Cultura?)
8- Política Nacional/ Elecciones

Teniendo en cuenta las tres pruebas realizadas con el modelo LDA, entendemos que el mejor modelo es el que plantea un K=5. ES DECIR, TODO LO QUE SIGUE ESTÁ PLANTEADO DESDE EL MODELO DE K=5.

# DUDA: en el ejemplo que aparece en la notebook, hay dos temas que no tienen un sentido definido y por eso "Esto parece un primer indicador de que deberíamos considerar la posibilidad de utilizar un número de tópicos más elevado". En nuestro caso lo estoy pensando al revés (menos k, mayor definición de tópicos).

La visualización correspondiente a k=5 muestra que algunas palabras como "argentina", "personas" son comunes a más de un tema. Es decir, que los tópicos identificados tienen cierta superposición en términos de palabras. Como alternativa, podríamos considerar los términos que tuvieran la mayor diferencia en β entre el tema 1 y el tema 5 (que son dos de los que mejor podemos interpretar). NO ME QUEDA DEL TODO CLARO, PERO ME PARECE QUE ESTE PASO PERMITE CORROBORAR QUE DOS TÓPICOS SEAN LO SUFICIENTEMENTE DIFERENTES. 

```{r}
beta_wide <- ap_topics_5 %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .002 | topic5 > .002) %>%
  mutate(log_ratio1_5 = log2(topic5 / topic1))

beta_wide
```
```{r}
beta_wide %>%
  ggplot(aes(x=reorder(term,log_ratio1_5) , y=log_ratio1_5)) +
    geom_col() +
    coord_flip() +
    labs(x='Término',
         y='Log2 ratio topic5/topic1') +
    theme_minimal()
```

Palabras como “vidal", "justicia" o "kirchner" caracterizan al tópico 1, mientras que “productos”, “inflación” o "mercado" representan al tópico 5. Esto contribuye a confirmar que se trata de dos tópicos diferenciados.


Composición de tópicos por documento:
Si antes estimamos cada tema como una mezcla de palabras, ahora usamos LDA también para modelar cada documento como una mezcla de temas. 

```{r}
doc_2_topics <- tidy(lda_v2_5, matrix = "gamma")
doc_2_topics %>%
  mutate(gamma = round(gamma, 5),
         document = as.integer(document)) %>%
  arrange(document, desc(gamma))
```
Cada uno de estos valores es una proporción estimada de palabras de ese documento que se generan a partir de ese tema.

```{r}
doc_2_topics %>%
  filter(topic == 1 |topic == 2 |topic == 3 |topic == 4 | topic == 5) %>% #creo que este paso no va porque en defitiva estoy filtrando todos los tópicos
  mutate(gamma = round(gamma, 5))
```
En particular, si observamos el documento 94, la probabilidad gamma asociada al tema 1 es 0.99878, lo que significa que el modelo estima que alrededor del 99% de las palabras en el documento 94 se generaron a partir del tema 1. Por lo tanto, podemos concluir que el tema 1 es altamente relevante para el documento 94 según el modelo.
Por otro lado, para el documento 43, la probabilidad gamma asociada al tema 1 es de 0.00018, lo que indica que menos del 1% de las palabras del documento 43 se generan a partir del tema 1. En esa dirección, podemos concluir que el tema 1 es poco relevante para el documento 43 según el modelo.

```{r}
corpus_tidy_v2%>%
  filter(id==94) %>%
  group_by(id, word) %>%
  summarise(n=n()) %>%
  select(word, n) %>%
  arrange(desc(n))
```
Se ve como en esta noticia parecen predominar palabras del tópico 1 ("nacional", "elecciones"). Veamos el texto completo de este documento:

```{r}
corpus_base %>%
  filter(id==94) %>%
  select(texto) %>%
  pull()
```
La noticia (id=94) habla de las elecciones y, en particular, de las elecciones en la provincia de Mendoza.

```{r}
doc_2_topics %>%
  rename(id = document) %>% # tenemos que renombrar la columna para que pueda hacerse el join
  mutate(id = as.integer(id)) %>%
  left_join(corpus_base %>% select(id, medio) %>% unique()) %>%
  group_by(medio, topic) %>%
    summarise(mean = mean(gamma)*100) %>%
  ggplot() +
    geom_col(aes(x=topic, y=mean, fill=medio), position='dodge') +
    theme_minimal()
```
Resultados (interpretación a desarrollar)
Es posible reconocer diferencias en la prevalencia de los tópicos para cada uno de los medios, con expceción del tópico "Economía" que muestra una prevalencia similar en cuatro medios.

1- Política Nacional/ Elecciones = Télam
2- Deportes = Crónica
3- Sociedad = La Nación
4- Política Internacinal = Infobae
5- Economía = Télam, Página 12, Perfil y Clarín


## Modelado de tópicos: STM

Para implementar un modelado de temas con STM necesitamos construir una matriz DFM.
```{r dfm}
disc_dfm <- words_tidy_v2 %>%
                cast_dfm(medio, word, n)

disc_dfm
```


------------------------------------------------------------------------

A continuación, seleccionar las noticias vinculadas a algún tópico
relevante (por ejemplo, “Elecciones”) y construir un clasificador para
predecir la orientación del diario. Utilizar alguno de los modelos de
clasificación vistos a lo largo de al Diplomatura (regresión logística,
random forest, etc.). Utilizar como features el “Spanish Billion Word
Corpus and Embeddings”, analizado en clase (pueden descargar el
embedding en formato .bin del link). ¿Qué resultados arroja el modelo?
¿Es posible mediante el texto de las noticias conocer la línea editorial
del diario? Generar las visualizaciones y tablas correspondientes para
una correcta evaluación del modelo.
