---
title: "Trabajo Final DiplomaCSC - Opción 2"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

## Carga de librerías a utilizar

```{r library}
library(tidyverse)
library(tidytext)
library(tm)
library(textstem)
library(patchwork)
library(wordcloud) 
```

## Etapa 1: carga de datos y preprocesamiento

El archivo que se adjunta consiste en un corpus de unas 7.000 noticias
scrapeadas entre julio y septiembre de 2019 de los siguientes medios de
circulación nacional:

Télam La Nación Clarín Perfil Infobae MinutoUno Página 12

Constituye una muestra aleatoria del corpus construido por Florencia
Piñeyrúa para su tesina de grado “Procesamiento del lenguaje natural
aplicado al estudio de tópicos de noticias de seguridad en Argentina:
julio a septiembre 2019”. Una exposición más concentrada de sus
resultados puede encontrarse en el siguiente artículo.

El corpus contiene, las siguientes variables:

id : identificador de cada documento

url : link a la noticia original

fecha : fecha de publicación

anio : año de publicación

mes : mes de publicación

dia : dia de publicación

medio : medio en el que fue publicado

orientacion: clasificación -provisoria- de los medios según su línea
editorial predominante (más conservador, más progresista, neutral)

titulo

texto

### Carga dataset

```{r readcsv}
corpus_base <- read_csv("M5_corpus_medios.csv")
```

```{r analisis_base}
# En este apartado vemos cuantas noticias aporta cada medio al corpus y calculamos la proporción sobre el total

    corpus_base %>%
        group_by(medio) %>%
        summarise(n=n()) %>%
        mutate(
                total = sum(n),
                prop = n/total*100
                ) %>%
        ungroup() %>%
        select(medio, n, prop) %>% 
        arrange(desc(n))
```

### Normalización y tokenización

```{r tokens}
corpus_tidy <- corpus_base %>% 
    mutate(texto= stringi::stri_trans_general(texto, "Latin-ASCII"),
         titulo = stringi::stri_trans_general(titulo, "Latin-ASCII")) %>% 
    mutate(texto = str_replace_all(texto, '[[:digit:]]+', '')) %>% 
    unnest_tokens(word, texto, to_lower = TRUE)

# En este este código tomamos el corpus de texto "corpus_base", lo normalizamos convirtiendo el texto y el título a caracteres ASCII, eliminamos los dígitos del texto y finalmente los dividimos en tokens para su posterior análisis.
```

### Eliminar stopwords

```{r stopwords}
stop_words_1 <- read_csv('https://raw.githubusercontent.com/Alir3z4/stop-words/master/spanish.txt', col_names=FALSE) %>%
        rename(word = X1) %>%
        mutate(word = stringi::stri_trans_general(word, "Latin-ASCII"))

# En este código leemos un archivo CSV de stop words en español ubicado en la URL especificada, renombramos la columna a "word" y luego transformamos las palabras en la columna para asegurarnos de que estén en formato ASCII. El resultado final es un dataframe llamado "stop_words_1" que contiene la lista de stopwords normalizadas.

stop_words_2 <- read_csv("z_stopwords.txt", col_names = FALSE) %>% 
  rename(word = X1) %>% 
   mutate(word=stringi::stri_trans_general(word, "Latin-ASCII"))

# Ídem pero el archivo CSV llamado "z_stopwords.txt" se carga desde la carpeta.

stop_words_full <- stop_words_1 %>% 
  bind_rows(stop_words_2) %>% 
  distinct()

# En este paso combinamos los dos dataframes de stopwords, eliminamos las filas duplicadas y guardamos el resultado en un nuevo dataframe llamado "stop_words_full". Este dataframe contiene la lista completa y única de stopwords normalizadas.

corpus_tidy <- corpus_tidy %>% 
  anti_join(stop_words_full)

# En este código eliminamos las stopwords del corpus de texto "corpus_tidy" utilizando la lista de stopwords contenida en el dataframe "stop_words_full". Este paso en el procesamiento de texto es necesario para eliminar palabras que no aportan significado para el análisis posterior.
```

### Corrección "años"

```{r corpus_tidy}
corpus_tidy <- corpus_tidy %>% 
  mutate(word = case_when(
    word == 'ano' ~ 'anio',
    word == 'anos' ~ 'anio',
    TRUE ~ word
  ))
```

```{r}
# PROBÉ UN MONTÓN DE ALTERNATIVAS, PERO EL CÓDIGO DE LA LEMATIZACIÓN SIGUE SIN FUNCIONAR (FIJATE QUE EL ELEMENTO CORPUS_TIDY_LEMM DUPLICA LAS MIMAS PALABRAS EN LA ÚLTIMA COLUMNA). ENTIENDO QUE PUEDE SER UN TEMA DEL DICCIONARIO QUE SE USA, PERO CON OTRAS OPCIONES TAMPOCO ME FUNCIONA
corpus_tidy_lemm <- corpus_tidy %>%
  mutate(word_lemmatized = lemmatize_words(word, language = "es"))



corpus_tidy_lemm %>% 
  filter(word != word_lemmatized) %>% 
  select(word, word_lemmatized)
```

## Etapa 2: Consignas

¿Cuáles son las palabras más utilizadas en cada uno de los medios?
¿Pueden verse diferencias? (Tener en cuenta las diferentes métricas
trabajadas en el curso: tf, tf-idf, etc.) Generar las visualizaciones
que considere más pertinentes para responder la pregunta

### Exploración palabras más frecuentes

```{r explore_freqs}
corpus_tidy %>%
        group_by(word, medio) %>%
        summarise(n=n()) %>%
        arrange(desc(n))

# Mediante este código agrupamos las palabras únicas en el corpus, contamos el número de ocurrencias de cada palabra y las ordenamos en función de su frecuencia de aparición, mostrando primero las palabras más frecuentes.

corpus_tidy %>%
  filter(medio == 'infobae')  %>% 
  group_by(word) %>%
  summarise(n=n()) %>%
  arrange(desc(n))
        

# Mirada rápida a las palabras más frecuentes de algunos medios

corpus_tidy %>%
        group_by(medio, word) %>%
        summarise(n=n()) %>%
        arrange(desc(n)) %>%
        pivot_wider(names_from = medio,
                    values_from = n)

# Este código calcula el recuento de ocurrencias de cada palabra en el corpus de texto agrupado según el medio de comunicación, y luego reorganiza estos resultados en un formato más ancho donde cada medio tiene sus recuentos de ocurrencias asociados para cada palabra.

```

```{r word_counts}
word_counts_10 <- corpus_tidy %>%
        group_by(medio, word) %>%
        summarise(n=n()) %>%
        slice_max(n, n = 10) %>% 
        ungroup()

# En este paso calculamos el recuento de ocurrencias de cada palabra en el corpus, agrupadas por la columna "medio", y almacena estos recuentos en un nuevo dataframe llamado "word_counts" para pasarle a la función de graficado

word_counts_all_10 <- corpus_tidy %>%
        group_by(word) %>%
        summarise(n=n()) %>%
        slice_max(n, n = 10) %>% 
        ungroup() %>% 
        mutate(medio = 'general')

# Aquí calculamos el recuento de ocurrencias de cada palabra en el corpus, sin agrupar por medio. Luego, seleccionamos las 10 palabras más comunes en todo el corpus. A estos resultados les agregamos la etiqueta "general" en la columna "medio" para poder comparar visualmente la distribución general con la distribución por medio

word_counts_top_10 <- word_counts_10 %>% 
  bind_rows(word_counts_all_10)

# Unifico en un dataset
```

```{r crear_viz}
crear_graf_word_freq <- function(data) {
  medios <- unique(data$medio)
  graficos <- list()
  
  for (medio_actual in medios) {
    datos_medio <- data %>%
      filter(medio == medio_actual) %>%
      mutate(word = fct_reorder(word, n))
    
    grafico <- ggplot(datos_medio, aes(n, word)) +
                geom_col() +
                geom_text(aes(label = n), position = position_stack(vjust = 0.5), family = "Courier", color = "white") +
                labs(title = medio_actual,
                     x = "Frecuencia",
                     y = "Palabra") +
                theme_classic()+
                theme(plot.title = element_text(hjust= 0.5),
                      axis.title = element_blank(),
                      axis.ticks.x = element_blank(),
                      text = element_text(family = "Courier"))
    
    graficos[[medio_actual]] <- grafico
  }
  
  wrap_plots(graficos)
}

# Esta función crea para cada medio un gráfico con el top 10 de palabras más frecuentes

```

El eje x muestra las palabras y el eje y muestra la frecuencia de cada
palabra. El gráfico está segmentado en paneles por cada medio, y las
escalas del eje y se ajustan para cada panel individual.

```{r viz}
crear_graf_word_freq(word_counts_top_10)

# Este código genera una visualización de las palabras más comunes en el corpus, desglosadas por medio y en general. Las palabras más comunes se muestran en orden descendente de frecuencia en cada categoría.


```

Algunas ideas hasta acá:

-   Clarín e Infobae aportan cada uno el 22% de las noticias al corpus.
    El peso de las palabras más importantes para estos medios en el
    ranking general es alto.

-   Año parece una stopword

-   Cronica, LN e Infobae son los únicos que no incluyen nombres propios
    entre las palabras más frecuentes

-   En LN hay muchas referencias a redes sociales y acciones en ellas
    (guardar, compartir). Creo que las acciones y "fuente" podrían ser
    stopwords. Incluso las menciones a RRSS podrían ser producto del
    scrapping y no del contenido de las notas

-   Macri el político más nombrado

-   Crónica e Infobae parecen cubrir temas más generales que los demás,
    aunque tienen el top 10 "gobierno" y "presidente"

-   Qué pasa con la palabra "frente" que aparece con frecuencia en
    varios medios? -\> validar si refiere a frente electoral o es
    stopword

### Term Frequency - TF:

A modo de práctica generamos el cálculo de term frequency manualmente
para visualizar a continuación la distribución de términos según su
frecuencia respecto al total de términos, aperturado por medio.

```{r tf}
# La "term frequency" (frecuencia de término) es una métrica para evaluar la importancia de una palabra en un documento o corpus de documentos, a partir de la medición de la frecuencia de su aparición en comparación con el número total de palabras.

words_tidy <- corpus_tidy %>% 
  group_by(medio, word) %>%
  summarise(n=n()) %>% 
  arrange(desc(n)) 

total_words <- words_tidy %>%
        group_by(medio) %>%
        summarize(total = sum(n)) 

words_tidy <- words_tidy %>% 
  left_join(total_words) %>% 
  ungroup() %>% 
  arrange(desc(n))

# En este código contamos la frecuencia de cada palabra en el corpus del medio y también el total de palabras en el medio. Con estos datos podemos calcular a continuación la importancia de cada término en el medio
```

```{r tf_viz}
words_tidy %>% mutate(tf = n/total) %>%
        ggplot(aes(tf, fill = medio)) +
                geom_histogram(show.legend = FALSE) +
                coord_flip()+
                xlim(NA, 0.0002) +
                facet_wrap(~medio) +
                theme_minimal()
```

Vemos que para los medios más masivos se cumple la Ley de Zipf: pocas
palabras ocurren muchas veces. Los corpus de Crónica, Minuto Uno y Télam
tienen menos términos que los demás y la tf tiene un rango menor (uso de
palabras mejor distribuido).

### Term Frequency - Inverse Document Frequency (TF-IDF)

*Nota-\> esta partecita de código que sigue no estoy segura que busca*

```{r}
# TF-IDF evalúa la importancia de una palabra en un documento dentro de un corpus más grande, considerando tanto la frecuencia de aparición de una palabra en un documento (TF) como la rareza de la palabra en el conjunto de documentos (IDF).
# La idea principal de esta métrica es que si una palabra aparece con frecuencia en un documento, pero también es rara en otros documentos, es probable que sea más relevante para ese documento en particular.

disc_dtm <- word_counts %>%
                cast_dtm(medio, word, n)

disc_dtm

# Como resultado se muestra una matriz de términos y documentos (DTM) con 8 documentos y 100.024 términos únicos (palabras) totales en el corpus. La DTM muestra entonces que existe una amplia variedad de términos únicos/ distintos en el corpus que expresaría una diversidad de vocabulario.

# Non-/sparse entries indica que de un total de 541.307 "entradas" posibles en la matriz (que es el número total de celdas), 258.885 tienen un valor distinto de cero, mientras que 541.307 - 258.885 = 282.422 "entradas" son cero.

# Sparsity indica la proporción de entradas que son cero en la matriz (cuanto mayor sea el valor de la sparsity, más dispersos son los datos). En nuestro caso, la sparsity significa que aproximadamente el 68% de las entradas en la matriz son cero. Este resultado indica que la matriz es bastante dispersa o "sparse", esto es, que hay muchas palabras que no aparecen en muchos documentos del corpus.
# NOTA: una alta sparsity puede presentar desafíos al analizar los datos. Por ejemplo, si estuviésemos buscando patrones de co-ocurrencia entre términos o realizando clustering de documentos, la alta sparsity puede hacer que sea más difícil encontrar relaciones significativas entre los términos y documentos.

# Maximal term length: Indica la longitud máxima de los términos (palabras) en el corpus. En nuestro caso es NA.

# Weighting muestra que el método utilizado para asignar un peso a cada término en la matriz es la term frequency - tf, lo que significa que el valor en cada celda de la matriz representa el número de veces que un término aparece en un documento específico.


term_freq_transposed <- word_counts_all

top_terms <- head(term_freq_transposed, 20)

set.seed(123)

wordcloud(words = top_terms$word, freq = as.numeric(top_terms$n), 
          min.freq = 5, scale = c(6, 0.5), random.order = FALSE, 
          colors = brewer.pal(8, "Dark2"), max.words = 150)
```

```{r tf_idf}
tf_idf <- words_tidy %>% 
  bind_tf_idf(word, medio, n)

tf_idf %>% select(-total) %>% 
  arrange(desc(tf_idf))

# En este código calculamos las métricas tf, idf y tf_idf para todos los términos del corpus
```

```{r tf_idf_viz}
tf_idf %>% 
  group_by(medio) %>% 
   slice_max(tf_idf, n = 10) %>%
        ungroup() %>%
        ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = medio)) +
        geom_col(show.legend = FALSE) +
        facet_wrap(~medio, ncol = 2, scales = "free") +
        labs(x = "tf-idf", y = NULL) +
        theme_minimal()

# Acá graficamos los términos más importantes e informativos para cada medio
```

Análisis de TF-IDF:

-   Lo primero que identifico es la necesidad de agregar palabras al
    listado de stopwords, porque no dicen nada sobre el
    contenido/orientación de las noticias y a su vez son únicos para
    algunos medios y eso genera un alto tf_idf

    Ejemplos: jpe, ap, emj, cronica.com.ar, fvazquez, cronicavirales,
    hd, pemex, gt, afv, jpg, minutouno.com, ambito.com, ivanovich,
    loading, paginai, protected, lxs, email, r.c,cp, fel,l.l,d.s, ea,
    f.f, f.d.s, fh, a.g, pct, telam.la, nacional.el

-   Podemos identificar "firmas" periodistas (nombres de usuario o
    siglas) para quitarlas del análisis de contenido

*Nota -\> Creo que en función de esta primera corrida de tf_idf
tendríamos que alargar el listado de stopwords para llegar a resultados
más limpios sobre los contenidos de cada medio, antes de intentar
identificar tópicos. Quizás podemos hacerlo dejando "guardado" todo lo
que hicimos hasta ahora para dar cuenta de este análisis, y mostrar el
resultado después d etoda esta "limpieza". Si estás de acuerdo avanzamos
así, engrosando stopwords pero conservando lo hecho hasta ahora, sin
pisar nada.*

### Ampliación stopwords

El análisis realizado hasta acá nos permitió identificar numerosas
palabras incluidas en el corpus producto del scrapping. Previo a la
modelización para identificar tópicos creemos pertinente ampliar el
listado de stopwords y repetir el preprocesamiento de los datos.

```{r}
tf_idf %>% select(word) %>% 
  arrange(desc(tf_idf))
```

```{r}

tf_idf %>% 
  select(word) %>% 
  filter(str_length(word) == 3 & !(word %in% c('san', 'mil', 'ley', 'afp', 'sur', 'fmi', 'usd', 'rio', 'gol', 'paz', 'mar', 'oro', 'red', 'luz', 'voz' , 'rol', 'sol', 'gas', 'pie', 'par', 'pro', 'via', 'onu', 'ypf', 'iva', 'afa', 'pbi', 'bar', 'cfk', 'eje', 'rey', 'atp', 'don', 'fbi', 'gay', 'psg', 'uba', 'ucr', 'ceo', 'ong', 'fed', 'ojo', 'dea', 'uva', 'cgt', 'ufi', 'app', 'gil', 'vih', 'nba', 'bbc', 'evo', 'hip', 'hop', 'fox', 'nbc', 'rap', 'adn', 'ala', 'eva', 'pan', 'zen', 'afv', 'cck', 'eco', 'oca', 'tio', 'cnn', 'cia', "dni", "uia", "fdt", "uif", "hbo", "mls", "oea", "mep", "gnc", "auh", "che", "oil", "gen", "agn",	"fpv", "lam",	"pib", "cne","duo", "vox", "dow", "uca", "dnu", "pez", "pfa", "pdt", "fda", "fci", "oms", "psa", "feo", "cta", "ego",				
"faa",	"ute", "ate",	"lio", "fpt", "suv", "gba",	"izq", "aro", "smn", "tnt",	"uco", "ipc", "saa", "tmz", "ccl", "gel", "vip", "esi",	"res", "kun", "tsj", "afi",	"pts", "cnh", "ajo", "acv", "bmw", "bus", "gps", "ile",	"ios", "unc",	"zoo", "jup", "tos", "unl",	"upl", "zeo", "ave", "mte", "mpn", "apn", "mao", "pba", "sms", "cnv", "mdz", "fol", "iso"))) %>% 
  pull(word)

tf_idf %>%
  filter(str_length(word) == 3) %>% select(word) %>% distinct()
```

```{r stopwords_v2}
#elimino palabras que incluyen puntos y parecen producto de scrapping web
nuevas_stopwords_1 <- tf_idf %>% 
  select(word) %>% 
  filter(grepl('\\.', word)) %>% 
  pull(word)

#elimino palabras de dos caracteres salvo contadas excepciones que tienen sentido
nuevas_stopwords_2 <- tf_idf %>% 
  select(word) %>% 
  filter(str_length(word) == 2 & !(word %in% c('pj', 'fe', 'dt', 'tv', 'km', 'cv', 'dr', 'it', 'dj', 'ux'))) %>% 
  pull(word)

#elimino palabras de tres caracteres. Parece haber más siglas y palabras cortas que tienen sentido pero dada la gran cantidad de stopwords de 3 caracteres hay motivos para incluir este paso
nuevas_stopwords_3 <- tf_idf %>% 
  select(word) %>% 
  filter(str_length(word) == 3 & !(word %in% c('san', 'mil', 'ley', 'afp', 'sur', 'fmi', 'usd', 'rio', 'gol', 'paz', 'mar', 'oro', 'red', 'luz', 'voz' , 'rol', 'sol', 'gas', 'pie', 'par', 'pro', 'via', 'onu', 'ypf', 'iva', 'afa', 'pbi', 'bar', 'cfk', 'eje', 'rey', 'atp', 'don', 'fbi', 'gay', 'psg', 'uba', 'ucr', 'ceo', 'ong', 'fed', 'ojo', 'dea', 'uva', 'cgt', 'ufi', 'app', 'gil', 'vih', 'nba', 'bbc', 'evo', 'hip', 'hop', 'fox', 'nbc', 'rap', 'adn', 'ala', 'eva', 'pan', 'zen', 'afv', 'cck', 'eco', 'oca', 'tio', 'cnn', 'cia', "dni", "uia", "fdt", "uif", "hbo", "mls", "oea", "mep", "gnc", "auh", "che", "oil", "gen", "agn",	"fpv", "lam",	"pib", "cne","duo", "vox", "dow", "uca", "dnu", "pez", "pfa", "pdt", "fda", "fci", "oms", "psa", "feo", "cta", "ego",				
"faa",	"ute", "ate",	"lio", "fpt", "suv", "gba",	"izq", "aro", "smn", "tnt",	"uco", "ipc", "saa", "tmz", "ccl", "gel", "vip", "esi",	"res", "kun", "tsj", "afi",	"pts", "cnh", "ajo", "acv", "bmw", "bus", "gps", "ile",	"ios", "unc",	"zoo", "jup", "tos", "unl",	"upl", "zeo", "ave", "mte", "mpn", "apn", "mao", "pba", "sms", "cnv", "mdz", "fol", "iso"))) %>% 
  pull(word)

nuevas_stopwords <- data.frame(word = nuevas_stopwords_1) %>% 
  bind_rows(data.frame(word = nuevas_stopwords_2)) %>% 
  bind_rows(data.frame(word = nuevas_stopwords_3))

stop_words_full_v2 <- stop_words_full %>% 
  bind_rows(tibble(word = c('embed', 'anio', 'gusta', 'twitter', 'facebook', 'comentar', 'fuente', 'whatsapp', 'guardar', 'compartir', 'mail', 'loading', 'email', 'paginai', 'eltrece', 'infobae' ))) %>% 
  bind_rows(nuevas_stopwords)
```

```{r corpus_tidy_v2}
corpus_tidy_v2 <- corpus_tidy %>% 
  anti_join(stop_words_full_v2)
```

```{r}
words_tidy_v2 <- corpus_tidy_v2 %>% 
  group_by(medio, word) %>%
  summarise(n=n())

```

```{r}
tf_idf_v2 <- words_tidy_v2 %>% 
  bind_tf_idf(word, medio, n)

tf_idf_v2 %>% 
  arrange(desc(tf_idf)) %>% 
  select(word) %>% 
  distinct()
```

------------------------------------------------------------------------

¿Cuáles son los tópicos principales en el corpus? ¿Pueden evidenciar
diferencias en cada uno de los medios? Explicar qué método se utilizó
para responder la pregunta, cuáles son los supuestos del mismo. Generar
las visualizaciones más adecuadas para responder a las preguntas

A continuación, seleccionar las noticias vinculadas a algún tópico
relevante (por ejemplo, “Elecciones”) y construir un clasificador para
predecir la orientación del diario. Utilizar alguno de los modelos de
clasificación vistos a lo largo de al Diplomatura (regresión logística,
random forest, etc.). Utilizar como features el “Spanish Billion Word
Corpus and Embeddings”, analizado en clase (pueden descargar el
embedding en formato .bin del link). ¿Qué resultados arroja el modelo?
¿Es posible mediante el texto de las noticias conocer la línea editorial
del diario? Generar las visualizaciones y tablas correspondientes para
una correcta evaluación del modelo.
