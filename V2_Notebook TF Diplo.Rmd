---
title: "Trabajo Final DiplomaCSC - Opción 2"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

## Carga de librerías a utilizar

```{r library}
library(tidyverse)
library(tidytext)
library(tm)
library(textstem)
library(patchwork)
library(wordcloud) 
library(plotly)
library(topicmodels)
library(tictoc)
library(cowplot)
library(reshape2)
```

## Etapa 1: carga de datos y preprocesamiento

El archivo que se adjunta consiste en un corpus de unas 7.000 noticias
scrapeadas entre julio y septiembre de 2019 de los siguientes medios de
circulación nacional:

Télam La Nación Clarín Perfil Infobae MinutoUno Página 12

Constituye una muestra aleatoria del corpus construido por Florencia
Piñeyrúa para su tesina de grado “Procesamiento del lenguaje natural
aplicado al estudio de tópicos de noticias de seguridad en Argentina:
julio a septiembre 2019”. Una exposición más concentrada de sus
resultados puede encontrarse en el siguiente artículo.

El corpus contiene las siguientes variables:

id : identificador de cada documento

url : link a la noticia original

fecha : fecha de publicación

anio : año de publicación

mes : mes de publicación

dia : dia de publicación

medio : medio en el que fue publicado

orientacion: clasificación -provisoria- de los medios según su línea
editorial predominante (más conservador, más progresista, neutral)

titulo

texto

### Carga dataset

```{r readcsv}
corpus_base <- read_csv("M5_corpus_medios.csv")
```

```{r analisis_base}
# En este apartado vemos cuántas noticias aporta cada medio al corpus y calculamos la proporción sobre el total.

    corpus_base %>%
        group_by(medio) %>%
        summarise(n=n()) %>%
        mutate(
                total = sum(n),
                prop = n/total*100
                ) %>%
        ungroup() %>%
        select(medio, n, prop) %>% 
        arrange(desc(n))
```

### Normalización y tokenización

```{r tokens}
corpus_tidy <- corpus_base %>% 
    mutate(texto= stringi::stri_trans_general(texto, "Latin-ASCII"),
         titulo = stringi::stri_trans_general(titulo, "Latin-ASCII")) %>% 
    mutate(texto = str_replace_all(texto, '[[:digit:]]+', '')) %>% 
    unnest_tokens(word, texto, to_lower = TRUE)

# En este este código tomamos el corpus de texto "corpus_base", lo normalizamos convirtiendo el texto y el título a caracteres ASCII, eliminamos los dígitos del texto y finalmente los dividimos en tokens para su posterior análisis.
```

### Eliminar stopwords

```{r stopwords}
stop_words_1 <- read_csv('https://raw.githubusercontent.com/Alir3z4/stop-words/master/spanish.txt', col_names=FALSE) %>%
        rename(word = X1) %>%
        mutate(word = stringi::stri_trans_general(word, "Latin-ASCII"))

# En este código leemos un archivo CSV de stop words en español ubicado en la URL especificada, renombramos la columna a "word" y luego transformamos las palabras en la columna para asegurarnos de que estén en formato ASCII. El resultado final es un dataframe llamado "stop_words_1" que contiene la lista de stopwords normalizadas.

stop_words_2 <- read_csv("z_stopwords.txt", col_names = FALSE) %>% 
  rename(word = X1) %>% 
   mutate(word=stringi::stri_trans_general(word, "Latin-ASCII"))

# Ídem pero el archivo CSV llamado "z_stopwords.txt" se carga desde la carpeta.

stop_words_full <- stop_words_1 %>% 
  bind_rows(stop_words_2) %>% 
  distinct()

# En este paso combinamos los dos dataframes de stopwords, eliminamos las filas duplicadas y guardamos el resultado en un nuevo dataframe llamado "stop_words_full". Este dataframe contiene la lista completa y única de stopwords normalizadas.

corpus_tidy <- corpus_tidy %>% 
  anti_join(stop_words_full)

# En este código eliminamos las stopwords del corpus de texto "corpus_tidy" utilizando la lista de stopwords contenida en el dataframe "stop_words_full". Este paso en el procesamiento de texto es necesario para eliminar palabras que no aportan significado para el análisis posterior.
```

### Corrección "años"

```{r corpus_tidy}
corpus_tidy <- corpus_tidy %>% 
  mutate(word = case_when(
    word == 'ano' ~ 'anio',
    word == 'anos' ~ 'anio',
    TRUE ~ word
  ))
```

```{r}
# PROBÉ UN MONTÓN DE ALTERNATIVAS, PERO EL CÓDIGO DE LA LEMATIZACIÓN SIGUE SIN FUNCIONAR (FIJATE QUE EL ELEMENTO CORPUS_TIDY_LEMM DUPLICA LAS MIMAS PALABRAS EN LA ÚLTIMA COLUMNA). ENTIENDO QUE PUEDE SER UN TEMA DEL DICCIONARIO QUE SE USA, PERO CON OTRAS OPCIONES TAMPOCO ME FUNCIONA
corpus_tidy_lemm <- corpus_tidy %>%
  mutate(word_lemmatized = lemmatize_words(word, language = "es"))

corpus_tidy_lemm %>% 
  filter(word != word_lemmatized) %>% 
  select(word, word_lemmatized)
```

## Etapa 2: Consignas

¿Cuáles son las palabras más utilizadas en cada uno de los medios?
¿Pueden verse diferencias? (Tener en cuenta las diferentes métricas
trabajadas en el curso: tf, tf-idf, etc.) Generar las visualizaciones
que considere más pertinentes para responder la pregunta.

### Exploración palabras más frecuentes

Comenzamos explorando el corpus en formato tidy para identificar
rápidamente algunos de los términos más frecuentes por medio.

```{r explore_freqs}
corpus_tidy %>%
        group_by(word, medio) %>%
        summarise(n=n()) %>%
        arrange(desc(n))

# Mediante este código agrupamos las palabras únicas en el corpus, contamos el número de ocurrencias de cada palabra y las ordenamos en función de su frecuencia de aparición, mostrando primero las palabras más frecuentes.

corpus_tidy %>%
  filter(medio == 'infobae')  %>% 
  group_by(word) %>%
  summarise(n=n()) %>%
  arrange(desc(n))
        
# Mirada rápida a las palabras más frecuentes de algunos medios

corpus_tidy %>%
        group_by(medio, word) %>%
        summarise(n=n()) %>%
        arrange(desc(n)) %>%
        pivot_wider(names_from = medio,
                    values_from = n)

# Este código calcula el recuento de ocurrencias de cada palabra en el corpus de texto agrupado según el medio de comunicación, y luego reorganiza estos resultados en un formato más ancho donde cada medio tiene sus recuentos de ocurrencias asociados para cada palabra.

```

```{r word_counts}
word_counts_10 <- corpus_tidy %>%
        group_by(medio, word) %>%
        summarise(n=n()) %>%
        slice_max(n, n = 10) %>% 
        ungroup()

# En este paso calculamos el recuento de ocurrencias de cada palabra en el corpus, agrupadas por la columna "medio", y almacena estos recuentos en un nuevo dataframe llamado "word_counts" para pasarle a la función de graficado.

word_counts_all_10 <- corpus_tidy %>%
        group_by(word) %>%
        summarise(n=n()) %>%
        slice_max(n, n = 10) %>% 
        ungroup() %>% 
        mutate(medio = 'general')

# Aquí calculamos el recuento de ocurrencias de cada palabra en el corpus, sin agrupar por medio. Luego, seleccionamos las 10 palabras más comunes en todo el corpus. A estos resultados les agregamos la etiqueta "general" en la columna "medio" para poder comparar visualmente la distribución general con la distribución por medio

word_counts_top_10 <- word_counts_10 %>% 
  bind_rows(word_counts_all_10)

# Unifico en un dataset
```

A continuación creamos una función para automatizar todas las
visualizaciones relacionadas a las métricas del corpus. COMENTARIO: CUÁLES SERÍAN LAS MÉTRICAS? EN TODO CASO, HASTA NO APARECE UNA SOLA MÉTRICA? (LA FRECUENCIA)

```{r crear_viz}
crear_graf_words <- function(data) {
  medios <- unique(data$medio)
  graficos <- list()
  
  for (medio_actual in medios) {
    datos_medio <- data %>%
      filter(medio == medio_actual) %>%
      mutate(word = fct_reorder(word, n))
    
    grafico <- ggplot(datos_medio, aes(n, word)) +
                geom_col() +
                geom_text(aes(label = n), position = position_stack(vjust = 0.5), family = "Courier", color = "white") +
                labs(title = medio_actual,
                     x = "Frecuencia",
                     y = "Palabra") +
                theme_classic()+
                theme(plot.title = element_text(hjust= 0.5),
                      axis.title = element_blank(),
                      axis.ticks.x = element_blank(),
                      text = element_text(family = "Courier"))
    
    graficos[[medio_actual]] <- grafico
  }
  
  wrap_plots(graficos)
}

# Esta función crea para cada medio un gráfico con el top 10 de palabras según cada métrica analizada

```

El eje x muestra las palabras y el eje y muestra la frecuencia de cada
palabra. El gráfico está segmentado en paneles por cada medio, y las
escalas del eje y se ajustan para cada panel individual.

```{r viz}
crear_graf_words(word_counts_top_10)

# Este código genera una visualización de las palabras más comunes en el corpus, desglosadas por medio y en general. Las palabras más comunes se muestran en orden descendente de frecuencia en cada categoría.
```

Algunas observaciones preliminares:

-   Clarín e Infobae aportan cada uno el 22% de las noticias al corpus. El peso de las palabras más importantes para estos medios en el ranking general es alto.

-   Año parece una stopword porque es una palabra que presenta una frecuencia alta intra e inter medios.

-   Crónica, LN e Infobae son los únicos que no incluyen nombres propios
entre las palabras más frecuentes.

-   En LN hay muchas referencias a redes sociales y acciones en ellas (guardar, compartir). Las acciones y "fuente" podrían ser stopwords. Incluso las menciones a RRSS podrían ser producto del scrapping y no del contenido de las notas. COMENTARIO: ESTO PARECERÍA SER ASÍ, NO? DIGO, QUE TAL VEZ SEA LA RESPUESTA QUE SE BUSCA, LA CORRECTA, PORQUE EN DEFINITIVA SE APLICARON LOS MISMOS CÓDIGOS A TODOS LOS MEDIOS. ENTONCES, SEGURAMENTE SEA COMO DECÍS, UN ERROR DEL SCRAPPING. COMENTARIO LUEGO DEL V2: YA QUEDÓ CORREGIDO EL CASO DE LN, PERO NO ENTIENDO CÓMO JAJA (DIGO, TENIENDO EN CUENTA EL COMENTARIO ANTERIOR QUE HICE).

-   Macri aparece como el político más nombrado. Lógicamente, es un resultado esperable si se tiene en cuenta que las notas relevadas corresponden al último año de su gobierno. 

-  Infobae y sobre todo Crónica parecen cubrir temas más generales que los demás medios, al incluir palabras como "polícía", "ciudad", "mujer", "hombre", "casa", "personas", "mundo", "vida". Incluso, la frecuencia de la palabra "policía" en Crónica (#2 o #1 si se excluye el término "anio") podría indicar que la mayor parte de la cobertura de este medio se orienta a la temática policial. De todas formas, en el top 10 también aparecen palabras como "gobierno" y "presidente".

-   La palabra "frente" aparece con frecuencia en varios medios, lo que
nos inclina a corroborar si refiere a frentes electorales o simplemente a un adverbio de lugar y, en este caso, debería considerarse como stopword.

-   A modo de síntesis, podría advertirse que las palabras con mayor frecuencia de aparición en la totalidad de los medios se corresponden con la temática política. Es probable que esta vinculación sea un desprendimiento del escenario electoral que signó el período bajo análisis.


### Term Frequency - TF:

A modo de práctica generamos el cálculo de term frequency manualmente
para visualizar la distribución de términos según su
frecuencia respecto al total de términos, aperturado por medio.

```{r tf_manual}
# La "term frequency" (frecuencia de término) es una métrica para evaluar la importancia de una palabra en un documento o corpus de documentos, a partir de la medición de la frecuencia de su aparición en comparación con el número total de palabras.

words_tidy <- corpus_tidy %>% 
  group_by(medio, word) %>%
  summarise(n=n()) %>% 
  arrange(desc(n)) 

total_words <- words_tidy %>%
        group_by(medio) %>%
        summarize(total = sum(n)) 

words_tidy <- words_tidy %>% 
  left_join(total_words) %>% 
  ungroup() %>% 
  arrange(desc(n))

# En este código contamos la frecuencia de cada palabra en el corpus del medio y también el total de palabras en el medio. Con estos datos podemos calcular a continuación la importancia de cada término en el medio.
```

En esta visualización podemos interactuar para identificar la cantidad
de términos por tf:

*Idea–\> agregar dato de términos totales por medio para comprender qué
peso tiene cada línea de tf/count de términos en cada corpus.*

```{r tf_viz_manual}
tf_viz <- words_tidy %>% mutate(tf = n/total) %>%
          ggplot(aes(tf)) +
                geom_histogram(show.legend = FALSE) +
                coord_flip()+
                xlim(NA, 0.0002) +
                facet_wrap(~medio) +
                theme_classic()+
                theme(plot.title = element_text(hjust= 0.5),
                      axis.title = element_blank(),
                      axis.ticks.x = element_blank(),
                      text = element_text(family = "Courier"))
                  
ggplotly(tf_viz)

```

Vemos que para los medios más masivos se cumple la Ley de Zipf: pocas
palabras ocurren muchas veces. Los corpus de Crónica, Minuto Uno y Télam
tienen menos términos que los demás y la tf tiene un rango menor (uso de
palabras mejor distribuido).


### Term Frequency - Inverse Document Frequency (TF-IDF)

En esta etapa complementamos el análisis con las métricas de idf y
tf_idf, para obtener una medida más completa de la importancia de los
términos en el corpus de documentos.

```{r tf_idf}
tf_idf <- words_tidy %>% 
  bind_tf_idf(word, medio, n)

tf_idf %>% select(-total) %>% 
  arrange(desc(tf_idf))

# En este código calculamos las métricas tf, idf y tf_idf para todos los términos del corpus
```

Creamos tres dataset, uno por métrica, con el top ten de términos para
pasarle a la función de visualización.

```{r top_ten_metricas}
tf_10 <- tf_idf %>%
        group_by(medio) %>%
        slice_max(tf, n = 10) %>% 
        select(medio, word, tf) %>% 
        rename(n = tf) %>%
        mutate(n = log(n + 1) * 100) %>% 
        mutate(n = round(n, 4))

idf_10 <- tf_idf %>%
          group_by(medio) %>%
          arrange(desc(idf)) %>%
          slice_head(n = 10) %>%
          select(medio, word, idf) %>%
          rename(n = idf) %>%
          mutate(n = log(n + 1) * 100) %>% 
          mutate(n = round(n, 4))


tf_idf_10 <- tf_idf %>%
        group_by(medio) %>%
        arrange(desc(idf)) %>%
        slice_head(n = 10) %>% 
        select(medio, word, tf_idf) %>% 
        rename(n = tf_idf) %>%
        mutate(n = log(n + 1) * 100) %>% 
        mutate(n = round(n, 4))
```

Pasamos cada uno de los top ten de términos por la función de graficado
para visualizar los términos más importantes según cada métrica.

```{r tf_viz}
crear_graf_words(tf_10)
```

```{r idf_viz}
crear_graf_words(idf_10)
```

```{r tf_idf_viz}
crear_graf_words(tf_idf_10)
```

Análisis preliminar de TF-IDF:

-   En primer lugar identificamos la necesidad de agregar palabras al
    listado de stopwords. Entendemos que no aportan información sobre el
    contenido u orientación de las noticias y a su vez son únicos para
    algunos medios y eso genera un alto tf_idf.

    Ejemplos: jpe, ap, emj, cronica.com.ar, fvazquez, cronicavirales,
    hd, pemex, gt, afv, jpg, minutouno.com, ambito.com, ivanovich,
    loading, paginai, protected, lxs, email, r.c,cp, fel,l.l,d.s, ea,
    f.f, f.d.s, fh, a.g, pct, telam.la, nacional.el

-   Podemos identificar "firmas" de periodistas (nombres de usuario o
    siglas) que tampoco son informativas sobre el contenido del corpus.

## Etapa 1.2: Ampliación stopwords y reprocesamiento de la base

El análisis realizado nos permitió encontrar numerosas palabras
incluidas en el corpus que creemos son producto del scrapping. Previo a
la modelización para identificar tópicos creemos pertinente ampliar el
listado de stopwords y repetir el preprocesamiento de los datos. COMENTARIO: EN REALIDAD, ESAS NUEVAS "STOPWORDS" NO TENDRÍAN MÁS QUE VER CON EL HECHO DE QUE SON TÉRMINOS MUY ESPECÍFICOS QUE NO FIGURAN EN LOS DICCIONARIOS QUE SE USAN PARA LIMPIAR LAS STOPWORDS? 

Para ampliar el listado de stopwords partimos del listado de palabras
con sus métricas de tf e idf.

```{r stopwords_v2}
# Mediante el siguiente código eliminamos palabras que incluyen puntos y parecen producto de scrapping web. COMENTARIO: ACÁ ME PERDÍ UN POCO: ESTA LIMPIEZA DE PUNTOS NO ESTABA HECHA EN LOS PRIMEROS PASOS DEL TP?
nuevas_stopwords_1 <- tf_idf %>% 
  select(word) %>% 
  filter(grepl('\\.', word)) %>% 
  pull(word)

# En este paso eliminamos palabras de dos caracteres, salvo contadas excepciones que tienen sentido.
nuevas_stopwords_2 <- tf_idf %>% 
  select(word) %>% 
  filter(str_length(word) == 2 & !(word %in% c('pj', 'fe', 'dt', 'tv', 'km', 'cv', 'dr', 'it', 'dj', 'ux'))) %>% 
  pull(word)

# Este código elimina palabras de tres caracteres. Parece haber más siglas y palabras cortas que tienen sentido pero, dada la gran cantidad de stopwords de 3 caracteres, hay motivos para incluir este paso.
nuevas_stopwords_3 <- tf_idf %>% 
  select(word) %>% 
  filter(str_length(word) == 3 & !(word %in% c('san', 'mil', 'ley', 'afp', 'sur', 'fmi', 'usd', 'rio', 'gol', 'paz', 'mar', 'oro', 'red', 'luz', 'voz' , 'rol', 'sol', 'gas', 'pie', 'par', 'pro', 'via', 'onu', 'ypf', 'iva', 'afa', 'pbi', 'bar', 'cfk', 'eje', 'rey', 'atp', 'don', 'fbi', 'gay', 'psg', 'uba', 'ucr', 'ceo', 'ong', 'fed', 'ojo', 'dea', 'uva', 'cgt', 'ufi', 'app', 'gil', 'vih', 'nba', 'bbc', 'evo', 'hip', 'hop', 'fox', 'nbc', 'rap', 'adn', 'ala', 'eva', 'pan', 'zen', 'afv', 'cck', 'eco', 'oca', 'tio', 'cnn', 'cia', "dni", "uia", "fdt", "uif", "hbo", "mls", "oea", "mep", "gnc", "auh", "che", "oil", "gen", "agn",	"fpv", "lam",	"pib", "cne","duo", "vox", "dow", "uca", "dnu", "pez", "pfa", "pdt", "fda", "fci", "oms", "psa", "feo", "cta", "ego",				
"faa",	"ute", "ate",	"lio", "fpt", "suv", "gba",	"izq", "aro", "smn", "tnt",	"uco", "ipc", "saa", "tmz", "ccl", "gel", "vip", "esi",	"res", "kun", "tsj", "afi",	"pts", "cnh", "ajo", "acv", "bmw", "bus", "gps", "ile",	"ios", "unc",	"zoo", "jup", "tos", "unl",	"upl", "zeo", "ave", "mte", "mpn", "apn", "mao", "pba", "sms", "cnv", "mdz", "fol", "iso"))) %>% 
  pull(word)

nuevas_stopwords <- data.frame(word = nuevas_stopwords_1) %>% 
  bind_rows(data.frame(word = nuevas_stopwords_2)) %>% 
  bind_rows(data.frame(word = nuevas_stopwords_3))

stop_words_full_v2 <- stop_words_full %>% 
  bind_rows(tibble(word = c('embed', 'anio', 'ano', 'anos', 'gusta', 'twitter', 'facebook', 'comentar', 'fuente', 'whatsapp', 'guardar', 'compartir', 'mail', 'loading', 'email', 'paginai', 'eltrece', 'infobae' ))) %>% 
  bind_rows(nuevas_stopwords)
```

A partir de la nueva lista de stopwords, recreamos el corpus en formato
tidy y también el dataframe en formato medio/término/n para continuar
con el análisis de métricas.

```{r corpus_tidy_v2}
corpus_tidy_v2 <- corpus_tidy %>% 
  anti_join(stop_words_full_v2)
```

```{r words_tidy_v2}
words_tidy_v2 <- corpus_tidy_v2 %>% 
  group_by(medio, word) %>%
  summarise(n=n())

total_words_v2 <- words_tidy_v2 %>%
        group_by(medio) %>%
        summarize(total = sum(n)) 

words_tidy_v2 <- words_tidy_v2 %>% 
  left_join(total_words_v2) %>% 
  ungroup() %>% 
  arrange(desc(n))
```

### Revisión de métricas

Creamos el gráfico con la distribución de términos según su tf para
identificar variaciones respecto a la v1.

```{r tf_viz_manual_v2}
tf_viz_v2 <- words_tidy_v2 %>% mutate(tf = n/total) %>%
          ggplot(aes(tf)) +
                geom_histogram(show.legend = FALSE) +
                coord_flip()+
                xlim(NA, 0.0002) +
                facet_wrap(~medio) +
                theme_classic()+
                theme(plot.title = element_text(hjust= 0.5),
                      axis.title = element_blank(),
                      axis.ticks.x = element_blank(),
                      text = element_text(family = "Courier"))
                  
ggplotly(tf_viz_v2)
```

*-\> falta comparar y análisis respecto a tf en v1. Comparar cuántos
terminos caen en la tf más cercana a 0 (terminos con baja importancia en
el corpus). A su vez, al quitar más stopwords deberíamos ver menos
términos cayendo en las tf más altas por ser palabras random que
aparecen pocas veces en el corpus*

# No se observan variaciones significativas entre v1 y v2 (ESTO LO DIGO A OJO, MIRANDO LOS DOS GRÁFICOS; ACÁ YA ME PERDÍ BASTANTE CON EL PLANTEO DE RECORTE DE LAS STOPWORDS).

```{r tf_idf_v2}
tf_idf_v2 <- words_tidy_v2 %>% 
  bind_tf_idf(word, medio, n)

tf_idf_v2 %>% 
  arrange(desc(tf_idf)) %>% 
  select(word) %>% 
  distinct()
```

Creamos los dataset necesarios para visualizar los principales términos
por métrica y por medio, buscando diferencias respecto a la v1.

```{r top_ten_metricas_v2}
tf_10_v2 <- tf_idf_v2 %>%
        group_by(medio) %>%
        slice_max(tf, n = 10) %>% 
        select(medio, word, tf) %>% 
        rename(n = tf) %>%
        mutate(n = log(n + 1) * 100) %>% 
        mutate(n = round(n, 4))

idf_10_v2 <- tf_idf_v2 %>%
          group_by(medio) %>%
          arrange(desc(idf)) %>%
          slice_head(n = 10) %>%
          select(medio, word, idf) %>%
          rename(n = idf) %>%
          mutate(n = log(n + 1) * 100) %>% 
          mutate(n = round(n, 4))


tf_idf_10_v2 <- tf_idf_v2 %>%
        group_by(medio) %>%
        arrange(desc(idf)) %>%
        slice_head(n = 10) %>% 
        select(medio, word, tf_idf) %>% 
        rename(n = tf_idf) %>%
        mutate(n = log(n + 1) * 100) %>% 
        mutate(n = round(n, 4))
```

```{r tf_viz_v2}
crear_graf_words(tf_10_v2)
```

```{r idf_viz_v2}
crear_graf_words(idf_10_v2)
```

```{r tf_idf_viz_v2}
crear_graf_words(tf_idf_10_v2)
```

Análisis final de TF-IDF:

*Comparar con análisis anterior y validar si estamos ok con los términos
más importantes del corpus para empezar con la detección de tópicos. A
simple vista parece que todavía podemos limpiar algunas palabras más,
incluyendolas en el listado de stopwords ampliado.*

-   En todos los medios, excepto en La Nación que no la incluía, logró eliminarse la palabra "anio" que figuraba en el primer lugar en términos de frecuencia. Además, en los casos de Crónica y Minuto 1, se eliminó otra palabra adicional incorporando, de esa manera, dos nuevas al top diez. 

- El caso de La Nación ya se muestra normalizado, sin las stopwords que aparecían producto del scraping.

- ESTO ES PARA REVISAR: En el conteo de Infobae de la v2 aparecen 11 palabras en vez de 10.

------------------------------------------------------------------------
## Modelado de tópicos: LDA

¿Cuáles son los tópicos principales en el corpus? ¿Pueden evidenciar
diferencias en cada uno de los medios? Explicar qué método se utilizó
para responder la pregunta, cuáles son los supuestos del mismo. Generar
las visualizaciones más adecuadas para responder a las preguntas.

Para implementar un modelado de temas con LDA necesitamos construir una matriz DTM.
```{r dtm}
disc_dtm <- words_tidy_v2 %>%
                cast_dtm(medio, word, n)

disc_dtm
```

```{r}
lda_5 <- LDA(disc_dtm, k = 5, control = list(seed = 1234))  # k es el número de tópicos a identificar

ap_topics <- tidy(lda_5, matrix = "beta") 

ap_topics %>%
  mutate(beta = round(100*beta,6))

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales='free_y') +
  scale_y_reordered() +
  theme_minimal()
```
Pareciera que los cinco tópicos no tienen un sentido tan definido. Entonces, cabe la posibilidad de que haga falta utilizar un número de tópicos más elevado.

A su vez, la visualización anterior muestra que algunas palabras como... son comunes a más de un tema. Es decir, que los tópicos identificados tienen cierta superposición en términos de palabras.Como alternativa, podríamos considerar los términos que tuvieran la mayor diferencia en β entre todos los temas.

```{r}

# ESTO NO SÉ SI ESTÁ BIEN APLICADO, EN LA NOTEBOOK EL CASO ERA ALGO DISTINTO. SE CALCULABA SOBRE LOS TÓPICOS MÁS DEFINIDOS, PERO EN NUESTRO CASO NINGUNO ES DEFINIDO.

# Con este código calculamos el logaritmo de la proporción de cada tópico respecto al tópico de referencia para los cinco tópicos del conjunto de datos. Cada columna log_ratio representa la diferencia logarítmica entre los tópicos adyacentes. Esto proporciona información sobre cómo cambia la contribución relativa de cada tópico en comparación con su tópico adyacente.

beta_wide3 <- ap_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>%
  filter(topic1 > 0.002 | topic2 > 0.002 | topic3 > 0.002 | topic4 > 0.002 | topic5 > 0.002) %>%  # Filtro para eliminar valores muy bajos
  mutate(log_ratio1_2 = log2(topic2 / topic1),
         log_ratio2_3 = log2(topic3 / topic2),
         log_ratio3_4 = log2(topic4 / topic3),
         log_ratio4_5 = log2(topic5 / topic4),
         log_ratio5_1 = log2(topic1 / topic5))

beta_wide3
```
```{r}
beta_wide3 %>%
  ggplot(aes(x=reorder(term,log_ratio1_2) , y=log_ratio1_2)) +
    geom_col() +
    coord_flip() +
    labs(x='Término',
         y='Log2 ratio topic3/topic2') +
    theme_minimal()
```

```{r}
beta_wide3 %>%
  ggplot(aes(x=reorder(term,log_ratio2_3) , y=log_ratio2_3)) +
    geom_col() +
    coord_flip() +
    labs(x='Término',
         y='Log2 ratio topic2/topic1') +
    theme_minimal()
```
```{r}
beta_wide3 %>%
  ggplot(aes(x=reorder(term,log_ratio3_4) , y=log_ratio3_4)) +
    geom_col() +
    coord_flip() +
    labs(x='Término',
         y='Log2 ratio topic4/topic3') +
    theme_minimal()
```
```{r}
beta_wide3 %>%
  ggplot(aes(x=reorder(term,log_ratio4_5) , y=log_ratio4_5)) +
    geom_col() +
    coord_flip() +
    labs(x='Término',
         y='Log2 ratio topic5/topic4') +
    theme_minimal()
```

```{r}
beta_wide3 %>%
  ggplot(aes(x=reorder(term, log_ratio5_1) , y= log_ratio5_1)) +
    geom_col() +
    coord_flip() +
    labs(x='Término',
         y='Log2 ratio topic1/topic5') +
    theme_minimal()
```
```
## Modelado de tópicos: STM

Para implementar un modelado de temas con STM necesitamos construir una matriz DFM.
```{r dfm}
disc_dfm <- words_tidy_v2 %>%
                cast_dfm(medio, word, n)

disc_dfm
```


------------------------------------------------------------------------

A continuación, seleccionar las noticias vinculadas a algún tópico
relevante (por ejemplo, “Elecciones”) y construir un clasificador para
predecir la orientación del diario. Utilizar alguno de los modelos de
clasificación vistos a lo largo de al Diplomatura (regresión logística,
random forest, etc.). Utilizar como features el “Spanish Billion Word
Corpus and Embeddings”, analizado en clase (pueden descargar el
embedding en formato .bin del link). ¿Qué resultados arroja el modelo?
¿Es posible mediante el texto de las noticias conocer la línea editorial
del diario? Generar las visualizaciones y tablas correspondientes para
una correcta evaluación del modelo.
