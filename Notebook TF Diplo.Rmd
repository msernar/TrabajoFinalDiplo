---
title: "Trabajo Final DiplomaCSC - Opción 2"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
--- 

## Carga de librerías a utilizar

```{r}
library(tidyverse)
library(tidytext)
library(tm)
library(textstem)
library(patchwork)

```
algo 
## Etapa 1: cargar de datos y preprocesamiento

El archivo que se adjunta consiste en un corpus de unas 7.000 noticias
scrapeadas entre julio y septiembre de 2019 de los siguientes medios de
circulación nacional:

Télam La Nación Clarín Perfil Infobae MinutoUno Página 12

Constituye una muestra aleatoria del corpus construido por Florencia
Piñeyrúa para su tesina de grado “Procesamiento del lenguaje natural
aplicado al estudio de tópicos de noticias de seguridad en Argentina:
julio a septiembre 2019”. Una exposición más concentrada de sus
resultados puede encontrarse en el siguiente artículo.

El corpus contiene, las siguientes variables: 
id : identificador de cada documento 
url : link a la noticia original 
fecha : fecha de publicación
anio : año de publicación 
mes : mes de publicación 
dia : dia de publicación 
medio : medio en el que fue publicado 
orientacion: clasificación -provisoria- de los medios según su línea editorial predominante (más conservador, más progresista, neutral) titulo 
texto

### Carga dataset
```{r}
corpus_base <- read_csv("M5_corpus_medios.csv")
```

### Normalización y tokenización
```{r}
corpus_tidy <- corpus_base %>% 
    mutate(texto= stringi::stri_trans_general(texto, "Latin-ASCII"),
         titulo = stringi::stri_trans_general(titulo, "Latin-ASCII")) %>% 
    mutate(texto = str_replace_all(texto, '[[:digit:]]+', '')) %>% 
    unnest_tokens(word, texto, to_lower = TRUE)

# En este este código tomamos el corpus de texto "corpus_base", lo normalizamos convirtiendo el texto y el título a caracteres ASCII, eliminamos los dígitos del texto y finalmente los dividimos en tokens para su posterior análisis.
```

### Eliminar stopwords
```{r}
stop_words_1 <- read_csv('https://raw.githubusercontent.com/Alir3z4/stop-words/master/spanish.txt', col_names=FALSE) %>%
        rename(word = X1) %>%
        mutate(word = stringi::stri_trans_general(word, "Latin-ASCII"))

# En este código leemos un archivo CSV de stop words en español ubicado en la URL especificada, renombramos la columna a "word" y luego transformamos las palabras en la columna para asegurarnos de que estén en formato ASCII. El resultado final es un dataframe llamado "stop_words_1" que contiene la lista de stopwords normalizadas.

stop_words_2 <- read_csv("z_stopwords.txt", col_names = FALSE) %>% 
  rename(word = X1) %>% 
   mutate(word=stringi::stri_trans_general(word, "Latin-ASCII"))

# Ídem pero el archivo CSV llamado "z_stopwords.txt" se carga desde la carpeta.

stop_words_full <- stop_words_1 %>% 
  bind_rows(stop_words_2) %>% 
  distinct()

# En este paso combinamos los dos dataframes de stopwords, eliminamos las filas duplicadas y guardamos el resultado en un nuevo dataframe llamado "stop_words_full". Este dataframe contiene la lista completa y única de stopwords normalizadas.

corpus_tidy <- corpus_tidy %>% 
  anti_join(stop_words_full)

# En este código eliminamos las stopwords del corpus de texto "corpus_tidy" utilizando la lista de stopwords contenida en el dataframe "stop_words_full". Este paso en el procesamiento de texto es necesario para eliminar palabras que no aportan significado para el análisis posterior.


# PROBÉ UN MONTÓN DE ALTERNATIVAS, PERO EL CÓDIGO DE LA LEMATIZACIÓN SIGUE SIN FUNCIONAR (FIJATE QUE EL ELEMENTO CORPUS_TIDY_LEMM DUPLICA LAS MIMAS PALABRAS EN LA ÚLTIMA COLUMNA). ENTIENDO QUE PUEDE SER UN TEMA DEL DICCIONARIO QUE SE USA, PERO CON OTRAS OPCIONES TAMPOCO ME FUNCIONA
corpus_tidy_lemm <- corpus_tidy %>%
  mutate(word_lemmatized = lemmatize_words(word, language = "es"))
```

### Exploración palabras más frecuentes
```{r}
corpus_tidy %>%
        group_by(word, medio) %>%
        summarise(n=n()) %>%
        arrange(desc(n))

# Mediante este código agrupamos las palabras únicas en el corpus, contamos el número de ocurrencias de cada palabra y las ordenamos en función de su frecuencia de aparición, mostrando primero las palabras más frecuentes.

```
```{r}
corpus_tidy %>%
        group_by(medio, word) %>%
        summarise(n=n()) %>%
        arrange(desc(n)) %>%
        pivot_wider(names_from = medio,
                    values_from = n)

# Este código calcula el recuento de ocurrencias de cada palabra en el corpus de texto agrupado según el medio de comunicación, y luego reorganiza estos resultados en un formato más ancho donde cada medio tiene sus recuentos de ocurrencias asociados para cada palabra.

```

## Etapa 2: consignas

¿Cuáles son las palabras más utilizadas en cada uno de los medios? ¿Pueden verse diferencias? (Tener en cuenta las diferentes métricas trabajadas en el curso: tf, tf-idf, etc.) Generar las visualizaciones que considere más pertinentes para responder la pregunta

### Term Frequency: 20 palabras más usadas por cada medio vs. 20 palabras generales
```{r}
word_counts <- corpus_tidy %>%
        group_by(medio, word) %>%
        summarise(n=n()) %>%
        ungroup()

word_counts_all <- corpus_tidy %>%
        group_by(word) %>%
        summarise(n=n()) %>%
        slice_max(n, n = 20) %>% 
        ungroup() %>% 
        mutate(medio = 'general')

word_counts_all %>% 
  bind_rows(word_counts) %>% 
  group_by(medio) %>% 
  slice_max(n, n = 20) %>% 
   mutate(word = fct_reorder(word, n)) %>% 
  ggplot(aes(word, n))+
  geom_col()+
  coord_flip()+
  facet_wrap(~medio, scales = "free_y")
  
 
 
# En este paso el código calcula el recuento de ocurrencias de cada palabra en el corpus de texto, agrupado por el identificador único de cada documento, y almacena estos recuentos en un nuevo dataframe llamado "word_counts". La utilidad radica en la posibilidad de analizar la frecuencia de palabras en cada documento individualmente.
```
```{r}
```


### TF-IDF
```{r}
disc_dtm <- word_counts %>%
                cast_dtm(id, word, n)

disc_dtm

# Este código considera los recuentos de ocurrencias de palabras en cada documento almacenados en el dataframe "word_counts" y los convierte en una matriz dispersa de términos del documento (DTM) llamada "disc_dtm". Esto es útil para el análisis de texto, donde se puede utilizar la matriz DTM como entrada para diversos modelos de aprendizaje automático o análisis de texto.
```



¿Cuáles son los tópicos principales en el corpus? ¿Pueden evidenciar diferencias en cada uno de los medios? Explicar qué método se utilizó para responder la pregunta, cuáles son los supuestos del mismo. Generar las visualizaciones más adecuadas para responder a las preguntas

A continuación, seleccionar las noticias vinculadas a algún tópico relevante (por ejemplo, “Elecciones”) y construir un clasificador para predecir la orientación del diario. Utilizar alguno de los modelos de clasificación vistos a lo largo de al Diplomatura (regresión logística, random forest, etc.). Utilizar como features el “Spanish Billion Word Corpus and Embeddings”, analizado en clase (pueden descargar el embedding en formato .bin del link). ¿Qué resultados arroja el modelo? ¿Es posible mediante el texto de las noticias conocer la línea editorial del diario? Generar las visualizaciones y tablas correspondientes para una correcta evaluación del modelo.
