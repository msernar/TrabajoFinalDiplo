---
title: "Trabajo Final DiplomaCSC - Opción 2"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
--- 

## Carga de librerías a utilizar

```{r}
library(tidyverse)
library(tidytext)
library(tm)
library(textstem)
library(patchwork)
library(wordcloud)
```

## Etapa 1: cargar de datos y preprocesamiento

El archivo que se adjunta consiste en un corpus de unas 7.000 noticias
scrapeadas entre julio y septiembre de 2019 de los siguientes medios de
circulación nacional:

Télam La Nación Clarín Perfil Infobae MinutoUno Página 12

Constituye una muestra aleatoria del corpus construido por Florencia
Piñeyrúa para su tesina de grado “Procesamiento del lenguaje natural
aplicado al estudio de tópicos de noticias de seguridad en Argentina:
julio a septiembre 2019”. Una exposición más concentrada de sus
resultados puede encontrarse en el siguiente artículo.

El corpus contiene, las siguientes variables: 
id : identificador de cada documento 
url : link a la noticia original 
fecha : fecha de publicación
anio : año de publicación 
mes : mes de publicación 
dia : dia de publicación 
medio : medio en el que fue publicado 
orientacion: clasificación -provisoria- de los medios según su línea editorial predominante (más conservador, más progresista, neutral) titulo 
texto

### Carga dataset
```{r}
corpus_base <- read_csv("M5_corpus_medios.csv")
```

### Normalización y tokenización
```{r}
corpus_tidy <- corpus_base %>% 
    mutate(texto= stringi::stri_trans_general(texto, "Latin-ASCII"),
         titulo = stringi::stri_trans_general(titulo, "Latin-ASCII")) %>% 
    mutate(texto = str_replace_all(texto, '[[:digit:]]+', '')) %>% 
    unnest_tokens(word, texto, to_lower = TRUE)

# En este este código tomamos el corpus de texto "corpus_base", lo normalizamos convirtiendo el texto y el título a caracteres ASCII, eliminamos los dígitos del texto y finalmente los dividimos en tokens para su posterior análisis.
```

### Eliminar stopwords
```{r}
stop_words_1 <- read_csv('https://raw.githubusercontent.com/Alir3z4/stop-words/master/spanish.txt', col_names=FALSE) %>%
        rename(word = X1) %>%
        mutate(word = stringi::stri_trans_general(word, "Latin-ASCII"))

# En este código leemos un archivo CSV de stop words en español ubicado en la URL especificada, renombramos la columna a "word" y luego transformamos las palabras en la columna para asegurarnos de que estén en formato ASCII. El resultado final es un dataframe llamado "stop_words_1" que contiene la lista de stopwords normalizadas.

stop_words_2 <- read_csv("z_stopwords.txt", col_names = FALSE) %>% 
  rename(word = X1) %>% 
   mutate(word=stringi::stri_trans_general(word, "Latin-ASCII"))

# Ídem pero el archivo CSV llamado "z_stopwords.txt" se carga desde la carpeta.

stop_words_full <- stop_words_1 %>% 
  bind_rows(stop_words_2) %>% 
  distinct()

# En este paso combinamos los dos dataframes de stopwords, eliminamos las filas duplicadas y guardamos el resultado en un nuevo dataframe llamado "stop_words_full". Este dataframe contiene la lista completa y única de stopwords normalizadas.

corpus_tidy <- corpus_tidy %>% 
  anti_join(stop_words_full)

# En este código eliminamos las stopwords del corpus de texto "corpus_tidy" utilizando la lista de stopwords contenida en el dataframe "stop_words_full". Este paso en el procesamiento de texto es necesario para eliminar palabras que no aportan significado para el análisis posterior.


# PROBÉ UN MONTÓN DE ALTERNATIVAS, PERO EL CÓDIGO DE LA LEMATIZACIÓN SIGUE SIN FUNCIONAR (FIJATE QUE EL ELEMENTO CORPUS_TIDY_LEMM DUPLICA LAS MIMAS PALABRAS EN LA ÚLTIMA COLUMNA). ENTIENDO QUE PUEDE SER UN TEMA DEL DICCIONARIO QUE SE USA, PERO CON OTRAS OPCIONES TAMPOCO ME FUNCIONA
corpus_tidy_lemm <- corpus_tidy %>%
  mutate(word_lemmatized = lemmatize_words(word, language = "es"))
```

### Exploración palabras más frecuentes
```{r}
corpus_tidy %>%
        group_by(word, medio) %>%
        summarise(n=n()) %>%
        arrange(desc(n))

# Mediante este código agrupamos las palabras únicas en el corpus, contamos el número de ocurrencias de cada palabra y las ordenamos en función de su frecuencia de aparición, mostrando primero las palabras más frecuentes.

```
```{r}
corpus_tidy %>%
        group_by(medio, word) %>%
        summarise(n=n()) %>%
        arrange(desc(n)) %>%
        pivot_wider(names_from = medio,
                    values_from = n)

# Este código calcula el recuento de ocurrencias de cada palabra en el corpus de texto agrupado según el medio de comunicación, y luego reorganiza estos resultados en un formato más ancho donde cada medio tiene sus recuentos de ocurrencias asociados para cada palabra.

```

## Etapa 2: consignas

¿Cuáles son las palabras más utilizadas en cada uno de los medios? ¿Pueden verse diferencias? (Tener en cuenta las diferentes métricas trabajadas en el curso: tf, tf-idf, etc.) Generar las visualizaciones que considere más pertinentes para responder la pregunta

### Term Frequency - TF: 20 palabras más usadas por cada medio vs. 20 palabras generales
```{r}
# La "term frequency" (frecuencia de término) es una métrica para evaluar la importancia de una palabra en un documento o corpus de documentos, a partir de la medición de la frecuencia de su aparición en comparación con el número total de palabras.
# Los siguientes códigos generan una visualización de las palabras más comunes en el corpus, desglosadas por medio y en general. Las palabras más comunes se muestran en orden descendente de frecuencia en cada categoría.

word_counts <- corpus_tidy %>%
        group_by(medio, word) %>%
        summarise(n=n()) %>%
        ungroup()

# En este paso calculamos el recuento de ocurrencias de cada palabra en el corpus, agrupadas por la columna "medio", y almacena estos recuentos en un nuevo dataframe llamado "word_counts".  

word_counts_all <- corpus_tidy %>%
        group_by(word) %>%
        summarise(n=n()) %>%
        slice_max(n, n = 20) %>% 
        ungroup() %>% 
        mutate(medio = 'general')

# Aquí calculamos el recuento de ocurrencias de cada palabra en el corpus, sin agrupar por medio. Luego, seleccionamos las 20 palabras más comunes en todo el corpus. A estos resultados les agregamos la etiqueta "general" en la columna "medio".

word_counts_all %>% 
  bind_rows(word_counts) %>% 
  group_by(medio) %>% 
  slice_max(n, n = 20) %>% 
   mutate(word = fct_reorder(word, n)) %>% 
  ggplot(aes(word, n))+
  geom_col()+
  coord_flip()+
  facet_wrap(~medio, scales = "free_y")
  
# Este código crea un gráfico de barras usando ggplot para visualizar las 20 palabras más comunes para cada medio y las 20 palabras más comunes en general. El eje x muestra las palabras y el eje y muestra la frecuencia de cada palabra. El gráfico está segmentado en paneles por cada medio, y las escalas del eje y se ajustan para cada panel individual. 
```

### Term Frequency - Inverse Document Frequency (TF-IDF)
```{r}
# TF-IDF evalúa la importancia de una palabra en un documento dentro de un corpus más grande, considerando tanto la frecuencia de aparición de una palabra en un documento (TF) como la rareza de la palabra en el conjunto de documentos (IDF).
# La idea principal de esta métrica es que si una palabra aparece con frecuencia en un documento, pero también es rara en otros documentos, es probable que sea más relevante para ese documento en particular.

disc_dtm <- word_counts %>%
                cast_dtm(medio, word, n)

disc_dtm

# Como resultado se muestra una matriz de términos y documentos (DTM) con 8 documentos y 100.024 términos únicos (palabras) totales en el corpus. La DTM muestra entonces que existe una amplia variedad de términos únicos/ distintos en el corpus que expresaría una diversidad de vocabulario.

# Non-/sparse entries indica que de un total de 541.307 "entradas" posibles en la matriz (que es el número total de celdas), 258.885 tienen un valor distinto de cero, mientras que 541.307 - 258.885 = 282.422 "entradas" son cero.

# Sparsity indica la proporción de entradas que son cero en la matriz (cuanto mayor sea el valor de la sparsity, más dispersos son los datos). En nuestro caso, la sparsity significa que aproximadamente el 68% de las entradas en la matriz son cero. Este resultado indica que la matriz es bastante dispersa o "sparse", esto es, que hay muchas palabras que no aparecen en muchos documentos del corpus.
# NOTA: una alta sparsity puede presentar desafíos al analizar los datos. Por ejemplo, si estuviésemos buscando patrones de co-ocurrencia entre términos o realizando clustering de documentos, la alta sparsity puede hacer que sea más difícil encontrar relaciones significativas entre los términos y documentos.

# Maximal term length: Indica la longitud máxima de los términos (palabras) en el corpus. En nuestro caso es NA.

# Weighting muestra que el método utilizado para asignar un peso a cada término en la matriz es la term frequency - tf, lo que significa que el valor en cada celda de la matriz representa el número de veces que un término aparece en un documento específico.


term_freq_transposed <- word_counts_all

top_terms <- head(term_freq_transposed, 20)

set.seed(123)

wordcloud(words = top_terms$word, freq = as.numeric(top_terms$n), 
          min.freq = 5, scale = c(6, 0.5), random.order = FALSE, 
          colors = brewer.pal(8, "Dark2"), max.words = 150)
```

¿Cuáles son los tópicos principales en el corpus? ¿Pueden evidenciar diferencias en cada uno de los medios? Explicar qué método se utilizó para responder la pregunta, cuáles son los supuestos del mismo. Generar las visualizaciones más adecuadas para responder a las preguntas

A continuación, seleccionar las noticias vinculadas a algún tópico relevante (por ejemplo, “Elecciones”) y construir un clasificador para predecir la orientación del diario. Utilizar alguno de los modelos de clasificación vistos a lo largo de al Diplomatura (regresión logística, random forest, etc.). Utilizar como features el “Spanish Billion Word Corpus and Embeddings”, analizado en clase (pueden descargar el embedding en formato .bin del link). ¿Qué resultados arroja el modelo? ¿Es posible mediante el texto de las noticias conocer la línea editorial del diario? Generar las visualizaciones y tablas correspondientes para una correcta evaluación del modelo.
